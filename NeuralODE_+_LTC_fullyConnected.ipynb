{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abhinash200301/Continuous-Time-Networks-for-Adaptive-Process-Control/blob/main/NeuralODE_%2B_LTC_fullyConnected.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79soaii8iRvz"
      },
      "source": [
        "# Imports and Setups"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ylFg6ScZhtsC",
        "outputId": "2e4e4b57-9e19-41e8-d5d6-1a04ae8a54af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow==2.13.0 in /usr/local/lib/python3.10/dist-packages (2.13.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.1.21 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (24.3.25)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (1.64.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (3.12.1)\n",
            "Requirement already satisfied: keras<2.14,>=2.13.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (2.13.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (18.1.1)\n",
            "Requirement already satisfied: numpy<=1.24.3,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (1.24.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.14,>=2.13 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (2.13.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (2.13.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (4.5.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (1.16.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.13.0) (0.44.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0) (3.7)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0) (3.0.6)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (3.2.2)\n",
            "Requirement already satisfied: keras==2.13.1 in /usr/local/lib/python3.10/dist-packages (2.13.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow==2.13.0\n",
        "!pip install keras==2.13.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KcF4Y_zFGKEa",
        "outputId": "42440e64-1e8a-479c-9976-c63c985499ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ncps\n",
            "  Downloading ncps-1.0.1-py3-none-any.whl.metadata (702 bytes)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from ncps) (1.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from ncps) (24.1)\n",
            "Downloading ncps-1.0.1-py3-none-any.whl (60 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/60.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.3/60.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ncps\n",
            "Successfully installed ncps-1.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install ncps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RjEkJt145zUk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from tensorflow.keras import layers, models, regularizers\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import r2_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxxU8Dwb9qeA",
        "outputId": "53e4c29d-a755-45e3-bba3-ed0f5875dcde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow_addons\n",
            "  Downloading tensorflow_addons-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow_addons) (24.1)\n",
            "Collecting typeguard<3.0.0,>=2.7 (from tensorflow_addons)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl.metadata (3.6 kB)\n",
            "Downloading tensorflow_addons-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (611 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.8/611.8 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: typeguard, tensorflow_addons\n",
            "  Attempting uninstall: typeguard\n",
            "    Found existing installation: typeguard 4.4.0\n",
            "    Uninstalling typeguard-4.4.0:\n",
            "      Successfully uninstalled typeguard-4.4.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "inflect 7.4.0 requires typeguard>=4.0.1, but you have typeguard 2.13.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tensorflow_addons-0.23.0 typeguard-2.13.3\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow_addons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NplcANUUGJKV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8940407-33b8-407b-e833-8317dd172442"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from ncps import wirings\n",
        "from ncps.tf import LTC, LTCCell\n",
        "import seaborn as sns\n",
        "from tensorflow_addons.metrics import RSquare"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hX09L5T4lh93"
      },
      "outputs": [],
      "source": [
        "import tensorflow_addons as tfa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JcHei9s3XB-t"
      },
      "outputs": [],
      "source": [
        "df= pd.read_csv(\"/content/PostDischarge_Data.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lr2BHEG0cfOv"
      },
      "outputs": [],
      "source": [
        "label=df[\"label\"]\n",
        "ct=[]\n",
        "for i in np.unique(label):\n",
        "  count=0\n",
        "  for j in range(len(df)):\n",
        "    if df[\"label\"][j]== i:\n",
        "      count=count+1\n",
        "  ct.append(count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9CcdXecU4k_h"
      },
      "outputs": [],
      "source": [
        "m=np.unique(label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hBnutLIBFEG",
        "outputId": "585aae9b-6e93-4c8d-b0d9-db0662dd8e92"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[349, 180, 201, 34, 328, 212, 140, 36, 325, 317, 298, 309]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "ct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cE4exGOQ_zyB",
        "outputId": "84d76a66-b858-4a60-d69c-4e875a0091a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "349\n",
            "529\n",
            "730\n",
            "764\n",
            "1092\n",
            "1304\n",
            "1444\n",
            "1480\n",
            "1805\n",
            "2122\n",
            "2420\n",
            "2729\n"
          ]
        }
      ],
      "source": [
        "l=0\n",
        "for i in range(len(ct)):\n",
        "  df1= df[l:l+ct[i]]\n",
        "  with pd.ExcelWriter(m[i]+\".xlsx\") as writer:\n",
        "    df1.to_excel(writer)\n",
        "  l= ct[i]+l\n",
        "  print(l)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZkbSw1_J6doA"
      },
      "outputs": [],
      "source": [
        "sequence_length = 5  # Length of the input sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k1KzHI_oDfrg"
      },
      "outputs": [],
      "source": [
        "# Get the list of Excel files\n",
        "a = glob.glob(\"*.xlsx\")\n",
        "\n",
        "# Lists to store inputs and outputs\n",
        "train_input = []\n",
        "train_output = []\n",
        "\n",
        "# Loop through each file\n",
        "for j in a:\n",
        "    dfk = pd.read_excel(j)\n",
        "    dfm = dfk.drop(columns=['label', 'cycle number', 'Unnamed: 0'])  # Drop unnecessary columns\n",
        "\n",
        "    # Loop to create sequences based on the specified sequence length\n",
        "    for i in range(len(dfk) - sequence_length):\n",
        "        ink = []\n",
        "        # Collecting the previous 'sequence_length' steps\n",
        "        for n in range(sequence_length):\n",
        "            ink.append(list(dfm[i + n:i + n + 1].values[0]))\n",
        "        train_input.append(ink)\n",
        "\n",
        "        # Predict the next value after the sequence\n",
        "        train_output.append(list(dfm[i + sequence_length:i + sequence_length + 1].values[0])[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OVtfZgrK6jeW"
      },
      "outputs": [],
      "source": [
        "# Convert lists to numpy arrays\n",
        "train_input = np.array(train_input)\n",
        "train_output = np.array(train_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zeqx6R6X9XGQ"
      },
      "outputs": [],
      "source": [
        "# Ensure train_output is reshaped to (samples, 1)\n",
        "train_output = train_output.reshape(-1, 1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "units = 32  # Number of units"
      ],
      "metadata": {
        "id": "J6NzKoEJCn4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dz99h2gAqW8w"
      },
      "outputs": [],
      "source": [
        "# Wiring: Fully connected Liquid Time Constant (LTC) with 1 motor neuron for output\n",
        "fc_wiring = wirings.FullyConnected(units, 1)  # 1 output neuron"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0aouUFXSqW5T"
      },
      "outputs": [],
      "source": [
        "model = keras.models.Sequential(\n",
        "    [\n",
        "        keras.layers.InputLayer(input_shape=(sequence_length, 121)),\n",
        "        LTC(fc_wiring, return_sequences=False),  # LTC Layer (returning only the last output)\n",
        "        # keras.layers.Dense(64, activation=\"relu\"),  # Dense layer with 64 units\n",
        "        # keras.layers.Dense(1, activation=\"linear\")  # Final output layer (predicting capacity)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Compile the model with R-squared as the key metric\n",
        "model.compile(optimizer=keras.optimizers.Adam(0.001), loss='mean_squared_error', metrics=[RSquare()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6YMVvu1sm1Ul",
        "outputId": "33104334-7d0e-432a-b9b6-a09e51cca7e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " ltc (LTC)                   (None, 1)                 19924     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 19924 (77.83 KB)\n",
            "Trainable params: 19924 (77.83 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Clearing any previous session\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# # Building and training the new model with Liquid Neural Network\n",
        "# model = get_model()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HBhpzX0jK7pX"
      },
      "outputs": [],
      "source": [
        "# # Set a larger figure size\n",
        "# sns.set_style(\"white\")\n",
        "# plt.figure(figsize=(50, 50))  # Adjust the figure size\n",
        "\n",
        "# # Draw the wiring with labels\n",
        "# legend_handles = fc_wiring.draw_graph(draw_labels=True)\n",
        "\n",
        "# # Add a legend\n",
        "# plt.legend(handles=legend_handles, loc=\"upper right\", bbox_to_anchor=(1.3, 1))\n",
        "\n",
        "# # Despine and adjust layout\n",
        "# sns.despine(left=True, bottom=True)\n",
        "# plt.tight_layout()\n",
        "\n",
        "# # Show the plot\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MAIkcxkpvlP"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GvUujlDKm1Tu",
        "outputId": "80019a38-de80-4209-e401-bf430751c7f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/250\n",
            "76/76 [==============================] - 17s 58ms/step - loss: 866.9719 - r_square: -29.2201 - val_loss: 1094.5244 - val_r_square: -162.2358\n",
            "Epoch 2/250\n",
            "76/76 [==============================] - 4s 47ms/step - loss: 837.0709 - r_square: -28.1774 - val_loss: 1060.2727 - val_r_square: -157.1276\n",
            "Epoch 3/250\n",
            "76/76 [==============================] - 6s 73ms/step - loss: 810.5453 - r_square: -27.2532 - val_loss: 1034.0662 - val_r_square: -153.2191\n",
            "Epoch 4/250\n",
            "76/76 [==============================] - 4s 48ms/step - loss: 787.2040 - r_square: -26.4396 - val_loss: 1010.5867 - val_r_square: -149.7175\n",
            "Epoch 5/250\n",
            "76/76 [==============================] - 4s 48ms/step - loss: 766.0949 - r_square: -25.7037 - val_loss: 987.0760 - val_r_square: -146.2111\n",
            "Epoch 6/250\n",
            "76/76 [==============================] - 5s 68ms/step - loss: 745.2336 - r_square: -24.9767 - val_loss: 962.8878 - val_r_square: -142.6037\n",
            "Epoch 7/250\n",
            "76/76 [==============================] - 4s 51ms/step - loss: 723.9526 - r_square: -24.2348 - val_loss: 937.4475 - val_r_square: -138.8096\n",
            "Epoch 8/250\n",
            "76/76 [==============================] - 4s 48ms/step - loss: 701.3682 - r_square: -23.4477 - val_loss: 908.6487 - val_r_square: -134.5146\n",
            "Epoch 9/250\n",
            "76/76 [==============================] - 4s 56ms/step - loss: 674.8206 - r_square: -22.5220 - val_loss: 873.6870 - val_r_square: -129.3004\n",
            "Epoch 10/250\n",
            "76/76 [==============================] - 5s 63ms/step - loss: 643.2276 - r_square: -21.4211 - val_loss: 835.1611 - val_r_square: -123.5547\n",
            "Epoch 11/250\n",
            "76/76 [==============================] - 4s 48ms/step - loss: 610.1644 - r_square: -20.2685 - val_loss: 795.9177 - val_r_square: -117.7020\n",
            "Epoch 12/250\n",
            "76/76 [==============================] - 4s 48ms/step - loss: 575.4862 - r_square: -19.0600 - val_loss: 754.7977 - val_r_square: -111.5695\n",
            "Epoch 13/250\n",
            "76/76 [==============================] - 5s 71ms/step - loss: 541.4615 - r_square: -17.8739 - val_loss: 715.3875 - val_r_square: -105.6919\n",
            "Epoch 14/250\n",
            "76/76 [==============================] - 4s 48ms/step - loss: 508.9246 - r_square: -16.7399 - val_loss: 677.4669 - val_r_square: -100.0365\n",
            "Epoch 15/250\n",
            "76/76 [==============================] - 4s 48ms/step - loss: 477.7536 - r_square: -15.6531 - val_loss: 640.4537 - val_r_square: -94.5163\n",
            "Epoch 16/250\n",
            "76/76 [==============================] - 5s 64ms/step - loss: 446.6410 - r_square: -14.5687 - val_loss: 602.4109 - val_r_square: -88.8427\n",
            "Epoch 17/250\n",
            "76/76 [==============================] - 4s 55ms/step - loss: 415.7746 - r_square: -13.4927 - val_loss: 565.8089 - val_r_square: -83.3839\n",
            "Epoch 18/250\n",
            "76/76 [==============================] - 4s 56ms/step - loss: 385.9962 - r_square: -12.4547 - val_loss: 529.0064 - val_r_square: -77.8953\n",
            "Epoch 19/250\n",
            "76/76 [==============================] - 4s 55ms/step - loss: 353.8610 - r_square: -11.3345 - val_loss: 484.6740 - val_r_square: -71.2836\n",
            "Epoch 20/250\n",
            "76/76 [==============================] - 5s 62ms/step - loss: 317.3503 - r_square: -10.0618 - val_loss: 440.5666 - val_r_square: -64.7055\n",
            "Epoch 21/250\n",
            "76/76 [==============================] - 4s 48ms/step - loss: 285.0939 - r_square: -8.9377 - val_loss: 403.9153 - val_r_square: -59.2394\n",
            "Epoch 22/250\n",
            "76/76 [==============================] - 5s 68ms/step - loss: 257.7671 - r_square: -7.9849 - val_loss: 371.0649 - val_r_square: -54.3401\n",
            "Epoch 23/250\n",
            "76/76 [==============================] - 5s 66ms/step - loss: 233.4223 - r_square: -7.1364 - val_loss: 341.0947 - val_r_square: -49.8704\n",
            "Epoch 24/250\n",
            "76/76 [==============================] - 4s 49ms/step - loss: 211.2585 - r_square: -6.3639 - val_loss: 313.3128 - val_r_square: -45.7270\n",
            "Epoch 25/250\n",
            "76/76 [==============================] - 4s 48ms/step - loss: 190.8431 - r_square: -5.6522 - val_loss: 287.2418 - val_r_square: -41.8388\n",
            "Epoch 26/250\n",
            "76/76 [==============================] - 5s 72ms/step - loss: 171.3694 - r_square: -4.9735 - val_loss: 259.9858 - val_r_square: -37.7739\n",
            "Epoch 27/250\n",
            "76/76 [==============================] - 4s 49ms/step - loss: 150.2887 - r_square: -4.2387 - val_loss: 230.4353 - val_r_square: -33.3668\n",
            "Epoch 28/250\n",
            "76/76 [==============================] - 4s 48ms/step - loss: 130.3716 - r_square: -3.5443 - val_loss: 205.9480 - val_r_square: -29.7148\n",
            "Epoch 29/250\n",
            "76/76 [==============================] - 5s 65ms/step - loss: 114.2027 - r_square: -2.9808 - val_loss: 184.4881 - val_r_square: -26.5143\n",
            "Epoch 30/250\n",
            "76/76 [==============================] - 4s 52ms/step - loss: 100.1799 - r_square: -2.4920 - val_loss: 165.4365 - val_r_square: -23.6730\n",
            "Epoch 31/250\n",
            "76/76 [==============================] - 4s 48ms/step - loss: 87.9370 - r_square: -2.0652 - val_loss: 148.2474 - val_r_square: -21.1094\n",
            "Epoch 32/250\n",
            "76/76 [==============================] - 4s 53ms/step - loss: 77.1575 - r_square: -1.6895 - val_loss: 132.9663 - val_r_square: -18.8304\n",
            "Epoch 33/250\n",
            "76/76 [==============================] - 5s 66ms/step - loss: 67.7602 - r_square: -1.3620 - val_loss: 119.1241 - val_r_square: -16.7660\n",
            "Epoch 34/250\n",
            "76/76 [==============================] - 4s 48ms/step - loss: 59.4342 - r_square: -1.0717 - val_loss: 106.6086 - val_r_square: -14.8995\n",
            "Epoch 35/250\n",
            "76/76 [==============================] - 4s 47ms/step - loss: 52.2716 - r_square: -0.8220 - val_loss: 95.5978 - val_r_square: -13.2573\n",
            "Epoch 36/250\n",
            "76/76 [==============================] - 5s 71ms/step - loss: 46.0052 - r_square: -0.6036 - val_loss: 85.6648 - val_r_square: -11.7759\n",
            "Epoch 37/250\n",
            "76/76 [==============================] - 4s 48ms/step - loss: 40.6103 - r_square: -0.4155 - val_loss: 76.7187 - val_r_square: -10.4417\n",
            "Epoch 38/250\n",
            "76/76 [==============================] - 4s 49ms/step - loss: 35.6811 - r_square: -0.2437 - val_loss: 68.7117 - val_r_square: -9.2476\n",
            "Epoch 39/250\n",
            "76/76 [==============================] - 5s 64ms/step - loss: 31.5946 - r_square: -0.1013 - val_loss: 61.5879 - val_r_square: -8.1851\n",
            "Epoch 40/250\n",
            "76/76 [==============================] - 4s 52ms/step - loss: 27.8568 - r_square: 0.0290 - val_loss: 55.2097 - val_r_square: -7.2339\n",
            "Epoch 41/250\n",
            "76/76 [==============================] - 4s 48ms/step - loss: 24.7597 - r_square: 0.1370 - val_loss: 49.6800 - val_r_square: -6.4092\n",
            "Epoch 42/250\n",
            "76/76 [==============================] - 4s 55ms/step - loss: 21.8753 - r_square: 0.2375 - val_loss: 44.6135 - val_r_square: -5.6536\n",
            "Epoch 43/250\n",
            "76/76 [==============================] - 5s 67ms/step - loss: 19.3467 - r_square: 0.3256 - val_loss: 40.0861 - val_r_square: -4.9784\n",
            "Epoch 44/250\n",
            "76/76 [==============================] - 5s 66ms/step - loss: 17.3360 - r_square: 0.3957 - val_loss: 36.2648 - val_r_square: -4.4085\n",
            "Epoch 45/250\n",
            "76/76 [==============================] - 6s 76ms/step - loss: 16.9646 - r_square: 0.4087 - val_loss: 33.0450 - val_r_square: -3.9283\n",
            "Epoch 46/250\n",
            "76/76 [==============================] - 4s 53ms/step - loss: 14.8433 - r_square: 0.4826 - val_loss: 30.1423 - val_r_square: -3.4954\n",
            "Epoch 47/250\n",
            "76/76 [==============================] - 4s 49ms/step - loss: 13.2639 - r_square: 0.5377 - val_loss: 27.5135 - val_r_square: -3.1033\n",
            "Epoch 48/250\n",
            "76/76 [==============================] - 4s 57ms/step - loss: 12.3758 - r_square: 0.5686 - val_loss: 25.3225 - val_r_square: -2.7766\n",
            "Epoch 49/250\n",
            "76/76 [==============================] - 5s 66ms/step - loss: 11.6396 - r_square: 0.5943 - val_loss: 23.4243 - val_r_square: -2.4935\n",
            "Epoch 50/250\n",
            "76/76 [==============================] - 4s 50ms/step - loss: 11.0025 - r_square: 0.6165 - val_loss: 21.7831 - val_r_square: -2.2487\n",
            "Epoch 51/250\n",
            "76/76 [==============================] - 4s 50ms/step - loss: 10.5905 - r_square: 0.6308 - val_loss: 20.4569 - val_r_square: -2.0509\n",
            "Epoch 52/250\n",
            "76/76 [==============================] - 7s 89ms/step - loss: 10.1551 - r_square: 0.6460 - val_loss: 19.2223 - val_r_square: -1.8668\n",
            "Epoch 53/250\n",
            "76/76 [==============================] - 4s 51ms/step - loss: 9.9494 - r_square: 0.6532 - val_loss: 18.2189 - val_r_square: -1.7171\n",
            "Epoch 54/250\n",
            "76/76 [==============================] - 4s 49ms/step - loss: 9.6201 - r_square: 0.6647 - val_loss: 17.2714 - val_r_square: -1.5758\n",
            "Epoch 55/250\n",
            "76/76 [==============================] - 5s 73ms/step - loss: 9.4870 - r_square: 0.6693 - val_loss: 16.6122 - val_r_square: -1.4775\n",
            "Epoch 56/250\n",
            "76/76 [==============================] - 4s 49ms/step - loss: 9.3127 - r_square: 0.6754 - val_loss: 16.0734 - val_r_square: -1.3972\n",
            "Epoch 57/250\n",
            "76/76 [==============================] - 4s 51ms/step - loss: 9.0742 - r_square: 0.6837 - val_loss: 15.7370 - val_r_square: -1.3470\n",
            "Epoch 58/250\n",
            "76/76 [==============================] - 5s 62ms/step - loss: 8.8675 - r_square: 0.6909 - val_loss: 15.3107 - val_r_square: -1.2834\n",
            "Epoch 59/250\n",
            "76/76 [==============================] - 4s 58ms/step - loss: 8.5509 - r_square: 0.7019 - val_loss: 14.8230 - val_r_square: -1.2107\n",
            "Epoch 60/250\n",
            "76/76 [==============================] - 4s 51ms/step - loss: 7.7617 - r_square: 0.7295 - val_loss: 14.5363 - val_r_square: -1.1679\n",
            "Epoch 61/250\n",
            "76/76 [==============================] - 4s 52ms/step - loss: 6.7001 - r_square: 0.7665 - val_loss: 11.9434 - val_r_square: -0.7812\n",
            "Epoch 62/250\n",
            "76/76 [==============================] - 5s 69ms/step - loss: 5.7578 - r_square: 0.7993 - val_loss: 10.8628 - val_r_square: -0.6201\n",
            "Epoch 63/250\n",
            "76/76 [==============================] - 4s 50ms/step - loss: 5.2800 - r_square: 0.8160 - val_loss: 9.3352 - val_r_square: -0.3922\n",
            "Epoch 64/250\n",
            "76/76 [==============================] - 4s 48ms/step - loss: 4.7189 - r_square: 0.8355 - val_loss: 8.3153 - val_r_square: -0.2401\n",
            "Epoch 65/250\n",
            "76/76 [==============================] - 6s 73ms/step - loss: 4.3226 - r_square: 0.8493 - val_loss: 7.4798 - val_r_square: -0.1155\n",
            "Epoch 66/250\n",
            "76/76 [==============================] - 4s 49ms/step - loss: 3.9682 - r_square: 0.8617 - val_loss: 6.7897 - val_r_square: -0.0126\n",
            "Epoch 67/250\n",
            "76/76 [==============================] - 4s 48ms/step - loss: 3.6930 - r_square: 0.8713 - val_loss: 6.1307 - val_r_square: 0.0857\n",
            "Epoch 68/250\n",
            "76/76 [==============================] - 5s 64ms/step - loss: 3.4008 - r_square: 0.8815 - val_loss: 5.5029 - val_r_square: 0.1793\n",
            "Epoch 69/250\n",
            "76/76 [==============================] - 4s 58ms/step - loss: 3.1632 - r_square: 0.8897 - val_loss: 5.1519 - val_r_square: 0.2317\n",
            "Epoch 70/250\n",
            "76/76 [==============================] - 4s 48ms/step - loss: 2.8952 - r_square: 0.8991 - val_loss: 4.4751 - val_r_square: 0.3326\n",
            "Epoch 71/250\n",
            "76/76 [==============================] - 4s 52ms/step - loss: 2.6795 - r_square: 0.9066 - val_loss: 5.1400 - val_r_square: 0.2334\n",
            "Epoch 72/250\n",
            "76/76 [==============================] - 5s 70ms/step - loss: 2.4875 - r_square: 0.9133 - val_loss: 4.2079 - val_r_square: 0.3724\n",
            "Epoch 73/250\n",
            "76/76 [==============================] - 4s 48ms/step - loss: 2.2995 - r_square: 0.9198 - val_loss: 3.6892 - val_r_square: 0.4498\n",
            "Epoch 74/250\n",
            "76/76 [==============================] - 4s 48ms/step - loss: 2.1351 - r_square: 0.9256 - val_loss: 3.0911 - val_r_square: 0.5390\n",
            "Epoch 75/250\n",
            "76/76 [==============================] - 5s 73ms/step - loss: 2.0244 - r_square: 0.9294 - val_loss: 4.0403 - val_r_square: 0.3974\n",
            "Epoch 76/250\n",
            "76/76 [==============================] - 4s 48ms/step - loss: 1.9236 - r_square: 0.9330 - val_loss: 2.6344 - val_r_square: 0.6071\n",
            "Epoch 77/250\n",
            "76/76 [==============================] - 4s 49ms/step - loss: 1.7317 - r_square: 0.9396 - val_loss: 3.9643 - val_r_square: 0.4088\n",
            "Epoch 78/250\n",
            "76/76 [==============================] - 5s 61ms/step - loss: 1.6758 - r_square: 0.9416 - val_loss: 2.4734 - val_r_square: 0.6311\n",
            "Epoch 79/250\n",
            "76/76 [==============================] - 5s 61ms/step - loss: 1.6532 - r_square: 0.9424 - val_loss: 2.8695 - val_r_square: 0.5720\n",
            "Epoch 80/250\n",
            "76/76 [==============================] - 4s 48ms/step - loss: 1.4805 - r_square: 0.9484 - val_loss: 2.1324 - val_r_square: 0.6820\n",
            "Epoch 81/250\n",
            "76/76 [==============================] - 4s 48ms/step - loss: 1.3927 - r_square: 0.9515 - val_loss: 2.5475 - val_r_square: 0.6201\n",
            "Epoch 82/250\n",
            "76/76 [==============================] - 5s 71ms/step - loss: 1.3166 - r_square: 0.9541 - val_loss: 2.5163 - val_r_square: 0.6247\n",
            "Epoch 83/250\n",
            "76/76 [==============================] - 4s 48ms/step - loss: 1.2372 - r_square: 0.9569 - val_loss: 1.6946 - val_r_square: 0.7473\n",
            "Epoch 84/250\n",
            "76/76 [==============================] - 4s 49ms/step - loss: 1.2099 - r_square: 0.9578 - val_loss: 2.3158 - val_r_square: 0.6546\n",
            "Epoch 85/250\n",
            "76/76 [==============================] - 5s 70ms/step - loss: 1.1690 - r_square: 0.9593 - val_loss: 1.8981 - val_r_square: 0.7169\n",
            "Epoch 86/250\n",
            "76/76 [==============================] - 4s 50ms/step - loss: 1.0995 - r_square: 0.9617 - val_loss: 2.0228 - val_r_square: 0.6983\n",
            "Epoch 87/250\n",
            "76/76 [==============================] - 4s 48ms/step - loss: 1.0463 - r_square: 0.9635 - val_loss: 1.6839 - val_r_square: 0.7489\n",
            "Epoch 88/250\n",
            "76/76 [==============================] - 4s 57ms/step - loss: 1.0113 - r_square: 0.9647 - val_loss: 1.5444 - val_r_square: 0.7697\n",
            "Epoch 89/250\n",
            "76/76 [==============================] - 5s 62ms/step - loss: 0.9495 - r_square: 0.9669 - val_loss: 1.7100 - val_r_square: 0.7450\n",
            "Epoch 90/250\n",
            "76/76 [==============================] - 4s 49ms/step - loss: 0.9241 - r_square: 0.9678 - val_loss: 1.3375 - val_r_square: 0.8005\n",
            "Epoch 91/250\n",
            "76/76 [==============================] - 4s 47ms/step - loss: 0.9006 - r_square: 0.9686 - val_loss: 2.0176 - val_r_square: 0.6991\n",
            "Epoch 92/250\n",
            "76/76 [==============================] - 5s 72ms/step - loss: 0.8830 - r_square: 0.9692 - val_loss: 2.0882 - val_r_square: 0.6886\n",
            "Epoch 93/250\n",
            "76/76 [==============================] - 4s 48ms/step - loss: 0.8368 - r_square: 0.9708 - val_loss: 1.4536 - val_r_square: 0.7832\n",
            "Epoch 94/250\n",
            "76/76 [==============================] - 4s 47ms/step - loss: 0.8189 - r_square: 0.9715 - val_loss: 1.9203 - val_r_square: 0.7136\n",
            "Epoch 95/250\n",
            "76/76 [==============================] - 5s 63ms/step - loss: 0.7930 - r_square: 0.9724 - val_loss: 2.1610 - val_r_square: 0.6777\n",
            "Epoch 96/250\n",
            "76/76 [==============================] - 4s 53ms/step - loss: 0.7689 - r_square: 0.9732 - val_loss: 1.8217 - val_r_square: 0.7283\n",
            "Epoch 97/250\n",
            "76/76 [==============================] - 4s 48ms/step - loss: 0.7456 - r_square: 0.9740 - val_loss: 2.4504 - val_r_square: 0.6345\n",
            "Epoch 98/250\n",
            "76/76 [==============================] - 4s 51ms/step - loss: 0.7204 - r_square: 0.9749 - val_loss: 2.4173 - val_r_square: 0.6395\n",
            "Epoch 99/250\n",
            "76/76 [==============================] - 5s 70ms/step - loss: 0.7045 - r_square: 0.9754 - val_loss: 1.6664 - val_r_square: 0.7515\n",
            "Epoch 100/250\n",
            "76/76 [==============================] - 4s 48ms/step - loss: 0.6614 - r_square: 0.9769 - val_loss: 1.2860 - val_r_square: 0.8082\n",
            "Epoch 101/250\n",
            "76/76 [==============================] - 4s 48ms/step - loss: 0.6538 - r_square: 0.9772 - val_loss: 1.4898 - val_r_square: 0.7778\n",
            "Epoch 102/250\n",
            "76/76 [==============================] - 5s 71ms/step - loss: 0.6307 - r_square: 0.9780 - val_loss: 1.1643 - val_r_square: 0.8264\n",
            "Epoch 103/250\n",
            "76/76 [==============================] - 4s 49ms/step - loss: 0.6183 - r_square: 0.9784 - val_loss: 1.8689 - val_r_square: 0.7213\n",
            "Epoch 104/250\n",
            "76/76 [==============================] - 4s 50ms/step - loss: 0.6242 - r_square: 0.9782 - val_loss: 1.3354 - val_r_square: 0.8008\n",
            "Epoch 105/250\n",
            "76/76 [==============================] - 5s 62ms/step - loss: 0.5937 - r_square: 0.9793 - val_loss: 2.4686 - val_r_square: 0.6318\n",
            "Epoch 106/250\n",
            "76/76 [==============================] - 5s 60ms/step - loss: 0.5656 - r_square: 0.9803 - val_loss: 1.6000 - val_r_square: 0.7614\n",
            "Epoch 107/250\n",
            "76/76 [==============================] - 4s 48ms/step - loss: 0.5746 - r_square: 0.9800 - val_loss: 1.8350 - val_r_square: 0.7263\n",
            "Epoch 108/250\n",
            "76/76 [==============================] - 4s 50ms/step - loss: 0.5131 - r_square: 0.9821 - val_loss: 2.0159 - val_r_square: 0.6993\n",
            "Epoch 109/250\n",
            "76/76 [==============================] - 5s 70ms/step - loss: 0.5095 - r_square: 0.9822 - val_loss: 1.5627 - val_r_square: 0.7669\n",
            "Epoch 110/250\n",
            "76/76 [==============================] - 4s 48ms/step - loss: 0.5926 - r_square: 0.9793 - val_loss: 1.7416 - val_r_square: 0.7403\n",
            "Epoch 111/250\n",
            "76/76 [==============================] - 4s 50ms/step - loss: 0.4924 - r_square: 0.9828 - val_loss: 1.3390 - val_r_square: 0.8003\n",
            "Epoch 112/250\n",
            "76/76 [==============================] - 6s 73ms/step - loss: 0.4965 - r_square: 0.9827 - val_loss: 1.6081 - val_r_square: 0.7602\n",
            "Epoch 113/250\n",
            "76/76 [==============================] - 4s 49ms/step - loss: 0.5079 - r_square: 0.9823 - val_loss: 1.3809 - val_r_square: 0.7941\n",
            "Epoch 114/250\n",
            "76/76 [==============================] - 4s 49ms/step - loss: 0.4790 - r_square: 0.9833 - val_loss: 0.9953 - val_r_square: 0.8516\n",
            "Epoch 115/250\n",
            "76/76 [==============================] - 5s 61ms/step - loss: 0.5353 - r_square: 0.9813 - val_loss: 1.8057 - val_r_square: 0.7307\n",
            "Epoch 116/250\n",
            "76/76 [==============================] - 5s 61ms/step - loss: 0.4559 - r_square: 0.9841 - val_loss: 1.6797 - val_r_square: 0.7495\n",
            "Epoch 117/250\n",
            "76/76 [==============================] - 4s 49ms/step - loss: 0.4525 - r_square: 0.9842 - val_loss: 1.5937 - val_r_square: 0.7623\n",
            "Epoch 118/250\n",
            "76/76 [==============================] - 4s 49ms/step - loss: 0.4494 - r_square: 0.9843 - val_loss: 1.0417 - val_r_square: 0.8446\n",
            "Epoch 119/250\n",
            "76/76 [==============================] - 6s 74ms/step - loss: 0.4213 - r_square: 0.9853 - val_loss: 1.3253 - val_r_square: 0.8023\n",
            "Epoch 120/250\n",
            "76/76 [==============================] - 4s 49ms/step - loss: 0.4176 - r_square: 0.9854 - val_loss: 1.5089 - val_r_square: 0.7750\n",
            "Epoch 121/250\n",
            "76/76 [==============================] - 4s 48ms/step - loss: 0.4436 - r_square: 0.9845 - val_loss: 1.9950 - val_r_square: 0.7025\n",
            "Epoch 122/250\n",
            "76/76 [==============================] - 6s 73ms/step - loss: 0.4246 - r_square: 0.9852 - val_loss: 1.4512 - val_r_square: 0.7836\n",
            "Epoch 123/250\n",
            "76/76 [==============================] - 4s 48ms/step - loss: 0.4103 - r_square: 0.9857 - val_loss: 1.0314 - val_r_square: 0.8462\n",
            "Epoch 124/250\n",
            "76/76 [==============================] - 4s 48ms/step - loss: 0.4122 - r_square: 0.9856 - val_loss: 1.6228 - val_r_square: 0.7580\n",
            "Epoch 125/250\n",
            "76/76 [==============================] - 4s 58ms/step - loss: 0.3980 - r_square: 0.9861 - val_loss: 1.6327 - val_r_square: 0.7565\n",
            "Epoch 126/250\n",
            "76/76 [==============================] - 5s 61ms/step - loss: 0.4049 - r_square: 0.9859 - val_loss: 2.1063 - val_r_square: 0.6859\n",
            "Epoch 127/250\n",
            "76/76 [==============================] - 4s 48ms/step - loss: 0.3968 - r_square: 0.9862 - val_loss: 1.4437 - val_r_square: 0.7847\n",
            "Epoch 128/250\n",
            "76/76 [==============================] - 4s 48ms/step - loss: 0.3984 - r_square: 0.9861 - val_loss: 1.4778 - val_r_square: 0.7796\n",
            "Epoch 129/250\n",
            "76/76 [==============================] - 5s 72ms/step - loss: 0.3756 - r_square: 0.9869 - val_loss: 1.1765 - val_r_square: 0.8245\n",
            "Epoch 130/250\n",
            "76/76 [==============================] - 4s 48ms/step - loss: 0.3840 - r_square: 0.9866 - val_loss: 1.4982 - val_r_square: 0.7766\n",
            "Epoch 131/250\n",
            "76/76 [==============================] - 4s 50ms/step - loss: 0.3672 - r_square: 0.9872 - val_loss: 1.6016 - val_r_square: 0.7611\n",
            "Epoch 132/250\n",
            "76/76 [==============================] - 5s 68ms/step - loss: 0.3559 - r_square: 0.9876 - val_loss: 1.4409 - val_r_square: 0.7851\n",
            "Epoch 133/250\n",
            "76/76 [==============================] - 4s 56ms/step - loss: 0.3721 - r_square: 0.9870 - val_loss: 1.4002 - val_r_square: 0.7912\n",
            "Epoch 134/250\n",
            "76/76 [==============================] - 4s 50ms/step - loss: 0.3693 - r_square: 0.9871 - val_loss: 2.1504 - val_r_square: 0.6793\n",
            "Epoch 135/250\n",
            "76/76 [==============================] - 4s 58ms/step - loss: 0.3909 - r_square: 0.9864 - val_loss: 2.8620 - val_r_square: 0.5732\n",
            "Epoch 136/250\n",
            "76/76 [==============================] - 5s 67ms/step - loss: 0.3720 - r_square: 0.9870 - val_loss: 2.4835 - val_r_square: 0.6296\n",
            "Epoch 137/250\n",
            "76/76 [==============================] - 4s 49ms/step - loss: 0.3653 - r_square: 0.9873 - val_loss: 1.2845 - val_r_square: 0.8084\n",
            "Epoch 138/250\n",
            "76/76 [==============================] - 4s 49ms/step - loss: 0.3338 - r_square: 0.9884 - val_loss: 1.2374 - val_r_square: 0.8155\n",
            "Epoch 139/250\n",
            "76/76 [==============================] - 5s 72ms/step - loss: 0.3591 - r_square: 0.9875 - val_loss: 1.6382 - val_r_square: 0.7557\n",
            "Epoch 140/250\n",
            "76/76 [==============================] - 4s 49ms/step - loss: 0.3588 - r_square: 0.9875 - val_loss: 1.7912 - val_r_square: 0.7329\n",
            "Epoch 141/250\n",
            "76/76 [==============================] - 4s 49ms/step - loss: 0.3457 - r_square: 0.9880 - val_loss: 2.3147 - val_r_square: 0.6548\n",
            "Epoch 142/250\n",
            "76/76 [==============================] - 5s 69ms/step - loss: 0.4435 - r_square: 0.9845 - val_loss: 1.2550 - val_r_square: 0.8128\n",
            "Epoch 143/250\n",
            "76/76 [==============================] - 4s 51ms/step - loss: 0.3475 - r_square: 0.9879 - val_loss: 1.1178 - val_r_square: 0.8333\n",
            "Epoch 144/250\n",
            "76/76 [==============================] - 4s 50ms/step - loss: 0.3509 - r_square: 0.9878 - val_loss: 1.5079 - val_r_square: 0.7751\n",
            "Epoch 145/250\n",
            "76/76 [==============================] - 5s 60ms/step - loss: 0.3349 - r_square: 0.9883 - val_loss: 1.0629 - val_r_square: 0.8415\n",
            "Epoch 146/250\n",
            "76/76 [==============================] - 5s 60ms/step - loss: 0.3443 - r_square: 0.9880 - val_loss: 2.0127 - val_r_square: 0.6998\n",
            "Epoch 147/250\n",
            "76/76 [==============================] - 4s 50ms/step - loss: 0.3706 - r_square: 0.9871 - val_loss: 2.3029 - val_r_square: 0.6566\n",
            "Epoch 148/250\n",
            "76/76 [==============================] - 4s 50ms/step - loss: 0.3476 - r_square: 0.9879 - val_loss: 2.0027 - val_r_square: 0.7013\n",
            "Epoch 149/250\n",
            "76/76 [==============================] - 5s 70ms/step - loss: 0.3590 - r_square: 0.9875 - val_loss: 1.0975 - val_r_square: 0.8363\n",
            "Epoch 150/250\n",
            "76/76 [==============================] - 4s 50ms/step - loss: 0.3566 - r_square: 0.9876 - val_loss: 1.6775 - val_r_square: 0.7498\n",
            "Epoch 151/250\n",
            "76/76 [==============================] - 4s 51ms/step - loss: 0.3148 - r_square: 0.9890 - val_loss: 1.4909 - val_r_square: 0.7776\n",
            "Epoch 152/250\n",
            "76/76 [==============================] - 5s 71ms/step - loss: 0.3212 - r_square: 0.9888 - val_loss: 1.9817 - val_r_square: 0.7045\n",
            "Epoch 153/250\n",
            "76/76 [==============================] - 4s 51ms/step - loss: 0.3211 - r_square: 0.9888 - val_loss: 2.0096 - val_r_square: 0.7003\n",
            "Epoch 154/250\n",
            "76/76 [==============================] - 4s 50ms/step - loss: 0.3129 - r_square: 0.9891 - val_loss: 1.0587 - val_r_square: 0.8421\n",
            "Epoch 155/250\n",
            "76/76 [==============================] - 6s 73ms/step - loss: 0.4363 - r_square: 0.9848 - val_loss: 1.5199 - val_r_square: 0.7733\n",
            "Epoch 156/250\n",
            "76/76 [==============================] - 4s 53ms/step - loss: 0.3060 - r_square: 0.9893 - val_loss: 1.4569 - val_r_square: 0.7827\n",
            "Epoch 157/250\n",
            "76/76 [==============================] - 4s 50ms/step - loss: 0.3017 - r_square: 0.9895 - val_loss: 1.5179 - val_r_square: 0.7736\n",
            "Epoch 158/250\n",
            "76/76 [==============================] - 5s 72ms/step - loss: 0.3135 - r_square: 0.9891 - val_loss: 1.7429 - val_r_square: 0.7401\n",
            "Epoch 159/250\n",
            "76/76 [==============================] - 4s 54ms/step - loss: 0.2995 - r_square: 0.9896 - val_loss: 2.0098 - val_r_square: 0.7003\n",
            "Epoch 160/250\n",
            "76/76 [==============================] - 4s 51ms/step - loss: 0.3207 - r_square: 0.9888 - val_loss: 2.2421 - val_r_square: 0.6656\n",
            "Epoch 161/250\n",
            "76/76 [==============================] - 5s 62ms/step - loss: 0.3012 - r_square: 0.9895 - val_loss: 2.7042 - val_r_square: 0.5967\n",
            "Epoch 162/250\n",
            "76/76 [==============================] - 5s 60ms/step - loss: 0.3154 - r_square: 0.9890 - val_loss: 2.0813 - val_r_square: 0.6896\n",
            "Epoch 163/250\n",
            "76/76 [==============================] - 4s 51ms/step - loss: 0.2996 - r_square: 0.9896 - val_loss: 2.3001 - val_r_square: 0.6570\n",
            "Epoch 164/250\n",
            "76/76 [==============================] - 4s 55ms/step - loss: 0.2993 - r_square: 0.9896 - val_loss: 2.0387 - val_r_square: 0.6960\n",
            "Epoch 165/250\n",
            "76/76 [==============================] - 5s 68ms/step - loss: 0.3053 - r_square: 0.9894 - val_loss: 1.1077 - val_r_square: 0.8348\n",
            "Epoch 166/250\n",
            "76/76 [==============================] - 4s 52ms/step - loss: 0.3025 - r_square: 0.9895 - val_loss: 1.6359 - val_r_square: 0.7560\n",
            "Epoch 167/250\n",
            "76/76 [==============================] - 4s 52ms/step - loss: 0.3023 - r_square: 0.9895 - val_loss: 2.3219 - val_r_square: 0.6537\n",
            "Epoch 168/250\n",
            "76/76 [==============================] - 6s 77ms/step - loss: 0.3149 - r_square: 0.9890 - val_loss: 1.4039 - val_r_square: 0.7906\n",
            "Epoch 169/250\n",
            "76/76 [==============================] - 4s 51ms/step - loss: 0.2955 - r_square: 0.9897 - val_loss: 1.0995 - val_r_square: 0.8360\n",
            "Epoch 170/250\n",
            "76/76 [==============================] - 4s 51ms/step - loss: 0.2941 - r_square: 0.9897 - val_loss: 2.1119 - val_r_square: 0.6850\n",
            "Epoch 171/250\n",
            "76/76 [==============================] - 6s 79ms/step - loss: 0.2941 - r_square: 0.9897 - val_loss: 1.6626 - val_r_square: 0.7520\n",
            "Epoch 172/250\n",
            "76/76 [==============================] - 4s 51ms/step - loss: 0.3023 - r_square: 0.9895 - val_loss: 1.6731 - val_r_square: 0.7505\n",
            "Epoch 173/250\n",
            "76/76 [==============================] - 4s 51ms/step - loss: 0.2884 - r_square: 0.9899 - val_loss: 2.0255 - val_r_square: 0.6979\n",
            "Epoch 174/250\n",
            "76/76 [==============================] - 5s 71ms/step - loss: 0.2990 - r_square: 0.9896 - val_loss: 1.9389 - val_r_square: 0.7108\n",
            "Epoch 175/250\n",
            "76/76 [==============================] - 4s 53ms/step - loss: 0.2946 - r_square: 0.9897 - val_loss: 2.4270 - val_r_square: 0.6380\n",
            "Epoch 176/250\n",
            "76/76 [==============================] - 4s 51ms/step - loss: 0.2949 - r_square: 0.9897 - val_loss: 1.6579 - val_r_square: 0.7527\n",
            "Epoch 177/250\n",
            "76/76 [==============================] - 5s 65ms/step - loss: 0.2869 - r_square: 0.9900 - val_loss: 1.2240 - val_r_square: 0.8174\n",
            "Epoch 178/250\n",
            "76/76 [==============================] - 4s 57ms/step - loss: 0.2985 - r_square: 0.9896 - val_loss: 1.8435 - val_r_square: 0.7251\n",
            "Epoch 179/250\n",
            "76/76 [==============================] - 4s 50ms/step - loss: 0.2913 - r_square: 0.9898 - val_loss: 2.3801 - val_r_square: 0.6450\n",
            "Epoch 180/250\n",
            "76/76 [==============================] - 4s 58ms/step - loss: 0.2899 - r_square: 0.9899 - val_loss: 1.1415 - val_r_square: 0.8298\n",
            "Epoch 181/250\n",
            "76/76 [==============================] - 5s 67ms/step - loss: 0.2907 - r_square: 0.9899 - val_loss: 1.8121 - val_r_square: 0.7297\n",
            "Epoch 182/250\n",
            "76/76 [==============================] - 4s 50ms/step - loss: 0.2904 - r_square: 0.9899 - val_loss: 1.2148 - val_r_square: 0.8188\n",
            "Epoch 183/250\n",
            "76/76 [==============================] - 4s 52ms/step - loss: 0.2690 - r_square: 0.9906 - val_loss: 1.6667 - val_r_square: 0.7514\n",
            "Epoch 184/250\n",
            "76/76 [==============================] - 6s 75ms/step - loss: 0.2722 - r_square: 0.9905 - val_loss: 1.4508 - val_r_square: 0.7836\n",
            "Epoch 185/250\n",
            "76/76 [==============================] - 4s 58ms/step - loss: 0.2753 - r_square: 0.9904 - val_loss: 2.5977 - val_r_square: 0.6126\n",
            "Epoch 186/250\n",
            "76/76 [==============================] - 6s 81ms/step - loss: 0.2725 - r_square: 0.9905 - val_loss: 2.3163 - val_r_square: 0.6546\n",
            "Epoch 187/250\n",
            "76/76 [==============================] - 5s 63ms/step - loss: 0.2603 - r_square: 0.9909 - val_loss: 2.1892 - val_r_square: 0.6735\n",
            "Epoch 188/250\n",
            "76/76 [==============================] - 4s 50ms/step - loss: 0.2721 - r_square: 0.9905 - val_loss: 2.1936 - val_r_square: 0.6728\n",
            "Epoch 189/250\n",
            "76/76 [==============================] - 4s 50ms/step - loss: 0.2885 - r_square: 0.9899 - val_loss: 1.8071 - val_r_square: 0.7305\n",
            "Epoch 190/250\n",
            "76/76 [==============================] - 6s 73ms/step - loss: 0.2719 - r_square: 0.9905 - val_loss: 2.1031 - val_r_square: 0.6863\n",
            "Epoch 191/250\n",
            "76/76 [==============================] - 4s 50ms/step - loss: 0.2649 - r_square: 0.9908 - val_loss: 1.6742 - val_r_square: 0.7503\n",
            "Epoch 192/250\n",
            "76/76 [==============================] - 4s 50ms/step - loss: 0.2979 - r_square: 0.9896 - val_loss: 2.4260 - val_r_square: 0.6382\n",
            "Epoch 193/250\n",
            "76/76 [==============================] - 6s 75ms/step - loss: 0.2869 - r_square: 0.9900 - val_loss: 2.3794 - val_r_square: 0.6451\n",
            "Epoch 194/250\n",
            "76/76 [==============================] - 4s 50ms/step - loss: 0.2723 - r_square: 0.9905 - val_loss: 1.6649 - val_r_square: 0.7517\n",
            "Epoch 195/250\n",
            "76/76 [==============================] - 4s 50ms/step - loss: 0.3031 - r_square: 0.9894 - val_loss: 2.2643 - val_r_square: 0.6623\n",
            "Epoch 196/250\n",
            "76/76 [==============================] - 5s 67ms/step - loss: 0.2756 - r_square: 0.9904 - val_loss: 1.7359 - val_r_square: 0.7411\n",
            "Epoch 197/250\n",
            "76/76 [==============================] - 4s 54ms/step - loss: 0.2833 - r_square: 0.9901 - val_loss: 1.3032 - val_r_square: 0.8056\n",
            "Epoch 198/250\n",
            "76/76 [==============================] - 4s 49ms/step - loss: 0.2744 - r_square: 0.9904 - val_loss: 1.5668 - val_r_square: 0.7663\n",
            "Epoch 199/250\n",
            "76/76 [==============================] - 4s 59ms/step - loss: 0.2494 - r_square: 0.9913 - val_loss: 2.0261 - val_r_square: 0.6978\n",
            "Epoch 200/250\n",
            "76/76 [==============================] - 5s 65ms/step - loss: 0.2448 - r_square: 0.9915 - val_loss: 2.3216 - val_r_square: 0.6538\n",
            "Epoch 201/250\n",
            "76/76 [==============================] - 4s 50ms/step - loss: 0.2511 - r_square: 0.9912 - val_loss: 2.5380 - val_r_square: 0.6215\n",
            "Epoch 202/250\n",
            "76/76 [==============================] - 4s 51ms/step - loss: 0.2608 - r_square: 0.9909 - val_loss: 1.4262 - val_r_square: 0.7873\n",
            "Epoch 203/250\n",
            "76/76 [==============================] - 6s 75ms/step - loss: 0.2515 - r_square: 0.9912 - val_loss: 2.2870 - val_r_square: 0.6589\n",
            "Epoch 204/250\n",
            "76/76 [==============================] - 4s 52ms/step - loss: 0.2470 - r_square: 0.9914 - val_loss: 1.4891 - val_r_square: 0.7779\n",
            "Epoch 205/250\n",
            "76/76 [==============================] - 4s 51ms/step - loss: 0.2461 - r_square: 0.9914 - val_loss: 2.8815 - val_r_square: 0.5703\n",
            "Epoch 206/250\n",
            "76/76 [==============================] - 5s 71ms/step - loss: 0.2565 - r_square: 0.9911 - val_loss: 2.3053 - val_r_square: 0.6562\n",
            "Epoch 207/250\n",
            "76/76 [==============================] - 4s 50ms/step - loss: 0.2430 - r_square: 0.9915 - val_loss: 1.8951 - val_r_square: 0.7174\n",
            "Epoch 208/250\n",
            "76/76 [==============================] - 4s 50ms/step - loss: 0.2784 - r_square: 0.9903 - val_loss: 1.5956 - val_r_square: 0.7620\n",
            "Epoch 209/250\n",
            "76/76 [==============================] - 5s 72ms/step - loss: 0.2511 - r_square: 0.9912 - val_loss: 2.0742 - val_r_square: 0.6907\n",
            "Epoch 210/250\n",
            "76/76 [==============================] - 4s 52ms/step - loss: 0.2499 - r_square: 0.9913 - val_loss: 1.9762 - val_r_square: 0.7053\n",
            "Epoch 211/250\n",
            "76/76 [==============================] - 4s 50ms/step - loss: 0.2400 - r_square: 0.9916 - val_loss: 1.6133 - val_r_square: 0.7594\n",
            "Epoch 212/250\n",
            "76/76 [==============================] - 5s 63ms/step - loss: 0.2709 - r_square: 0.9906 - val_loss: 1.3253 - val_r_square: 0.8023\n",
            "Epoch 213/250\n",
            "76/76 [==============================] - 4s 58ms/step - loss: 0.2425 - r_square: 0.9915 - val_loss: 1.8283 - val_r_square: 0.7273\n",
            "Epoch 214/250\n",
            "76/76 [==============================] - 4s 49ms/step - loss: 0.2585 - r_square: 0.9910 - val_loss: 1.7325 - val_r_square: 0.7416\n",
            "Epoch 215/250\n",
            "76/76 [==============================] - 4s 54ms/step - loss: 0.2365 - r_square: 0.9918 - val_loss: 2.3125 - val_r_square: 0.6551\n",
            "Epoch 216/250\n",
            "76/76 [==============================] - 6s 73ms/step - loss: 0.2512 - r_square: 0.9912 - val_loss: 1.1962 - val_r_square: 0.8216\n",
            "Epoch 217/250\n",
            "76/76 [==============================] - 4s 51ms/step - loss: 0.2650 - r_square: 0.9908 - val_loss: 1.9766 - val_r_square: 0.7052\n",
            "Epoch 218/250\n",
            "76/76 [==============================] - 4s 50ms/step - loss: 0.2798 - r_square: 0.9902 - val_loss: 1.8191 - val_r_square: 0.7287\n",
            "Epoch 219/250\n",
            "76/76 [==============================] - 6s 76ms/step - loss: 0.2339 - r_square: 0.9918 - val_loss: 1.7865 - val_r_square: 0.7336\n",
            "Epoch 220/250\n",
            "76/76 [==============================] - 4s 51ms/step - loss: 0.2532 - r_square: 0.9912 - val_loss: 1.3043 - val_r_square: 0.8055\n",
            "Epoch 221/250\n",
            "76/76 [==============================] - 4s 51ms/step - loss: 0.2294 - r_square: 0.9920 - val_loss: 2.0612 - val_r_square: 0.6926\n",
            "Epoch 222/250\n",
            "76/76 [==============================] - 6s 74ms/step - loss: 0.3155 - r_square: 0.9890 - val_loss: 2.2800 - val_r_square: 0.6600\n",
            "Epoch 223/250\n",
            "76/76 [==============================] - 4s 52ms/step - loss: 0.2322 - r_square: 0.9919 - val_loss: 1.4770 - val_r_square: 0.7797\n",
            "Epoch 224/250\n",
            "76/76 [==============================] - 4s 51ms/step - loss: 0.2318 - r_square: 0.9919 - val_loss: 2.0205 - val_r_square: 0.6987\n",
            "Epoch 225/250\n",
            "76/76 [==============================] - 5s 64ms/step - loss: 0.2306 - r_square: 0.9920 - val_loss: 3.7511 - val_r_square: 0.4406\n",
            "Epoch 226/250\n",
            "76/76 [==============================] - 5s 64ms/step - loss: 0.2545 - r_square: 0.9911 - val_loss: 4.0441 - val_r_square: 0.3969\n",
            "Epoch 227/250\n",
            "76/76 [==============================] - 4s 51ms/step - loss: 0.2719 - r_square: 0.9905 - val_loss: 1.5011 - val_r_square: 0.7761\n",
            "Epoch 228/250\n",
            "76/76 [==============================] - 4s 57ms/step - loss: 0.2365 - r_square: 0.9918 - val_loss: 2.0858 - val_r_square: 0.6889\n",
            "Epoch 229/250\n",
            "76/76 [==============================] - 5s 70ms/step - loss: 0.2205 - r_square: 0.9923 - val_loss: 1.6100 - val_r_square: 0.7599\n",
            "Epoch 230/250\n",
            "76/76 [==============================] - 4s 51ms/step - loss: 0.2304 - r_square: 0.9920 - val_loss: 1.5055 - val_r_square: 0.7755\n",
            "Epoch 231/250\n",
            "76/76 [==============================] - 4s 51ms/step - loss: 0.2584 - r_square: 0.9910 - val_loss: 1.3966 - val_r_square: 0.7917\n",
            "Epoch 232/250\n",
            "76/76 [==============================] - 6s 73ms/step - loss: 0.2282 - r_square: 0.9920 - val_loss: 1.8958 - val_r_square: 0.7173\n",
            "Epoch 233/250\n",
            "76/76 [==============================] - 4s 53ms/step - loss: 0.2573 - r_square: 0.9910 - val_loss: 1.7382 - val_r_square: 0.7408\n",
            "Epoch 234/250\n",
            "76/76 [==============================] - 4s 54ms/step - loss: 0.2282 - r_square: 0.9920 - val_loss: 2.0001 - val_r_square: 0.7017\n",
            "Epoch 235/250\n",
            "76/76 [==============================] - 6s 77ms/step - loss: 0.2310 - r_square: 0.9919 - val_loss: 1.5643 - val_r_square: 0.7667\n",
            "Epoch 236/250\n",
            "76/76 [==============================] - 4s 52ms/step - loss: 0.2254 - r_square: 0.9921 - val_loss: 2.0693 - val_r_square: 0.6914\n",
            "Epoch 237/250\n",
            "76/76 [==============================] - 4s 52ms/step - loss: 0.2257 - r_square: 0.9921 - val_loss: 2.2645 - val_r_square: 0.6623\n",
            "Epoch 238/250\n",
            "76/76 [==============================] - 5s 72ms/step - loss: 0.2457 - r_square: 0.9914 - val_loss: 1.2729 - val_r_square: 0.8102\n",
            "Epoch 239/250\n",
            "76/76 [==============================] - 4s 52ms/step - loss: 0.3539 - r_square: 0.9877 - val_loss: 2.1622 - val_r_square: 0.6775\n",
            "Epoch 240/250\n",
            "76/76 [==============================] - 4s 51ms/step - loss: 0.2187 - r_square: 0.9924 - val_loss: 1.7086 - val_r_square: 0.7452\n",
            "Epoch 241/250\n",
            "76/76 [==============================] - 5s 65ms/step - loss: 0.2184 - r_square: 0.9924 - val_loss: 1.7435 - val_r_square: 0.7400\n",
            "Epoch 242/250\n",
            "76/76 [==============================] - 5s 60ms/step - loss: 0.2194 - r_square: 0.9924 - val_loss: 1.5496 - val_r_square: 0.7689\n",
            "Epoch 243/250\n",
            "76/76 [==============================] - 4s 51ms/step - loss: 0.2233 - r_square: 0.9922 - val_loss: 1.8622 - val_r_square: 0.7223\n",
            "Epoch 244/250\n",
            "76/76 [==============================] - 4s 57ms/step - loss: 0.2306 - r_square: 0.9920 - val_loss: 1.7296 - val_r_square: 0.7420\n",
            "Epoch 245/250\n",
            "76/76 [==============================] - 5s 68ms/step - loss: 0.2119 - r_square: 0.9926 - val_loss: 1.7740 - val_r_square: 0.7354\n",
            "Epoch 246/250\n",
            "76/76 [==============================] - 4s 51ms/step - loss: 0.2143 - r_square: 0.9925 - val_loss: 1.9868 - val_r_square: 0.7037\n",
            "Epoch 247/250\n",
            "76/76 [==============================] - 4s 51ms/step - loss: 0.2310 - r_square: 0.9919 - val_loss: 1.8745 - val_r_square: 0.7204\n",
            "Epoch 248/250\n",
            "76/76 [==============================] - 6s 78ms/step - loss: 0.2304 - r_square: 0.9920 - val_loss: 1.9840 - val_r_square: 0.7041\n",
            "Epoch 249/250\n",
            "76/76 [==============================] - 4s 51ms/step - loss: 0.2281 - r_square: 0.9920 - val_loss: 1.5875 - val_r_square: 0.7632\n",
            "Epoch 250/250\n",
            "76/76 [==============================] - 4s 51ms/step - loss: 0.2261 - r_square: 0.9921 - val_loss: 1.4841 - val_r_square: 0.7787\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(train_input, train_output, epochs=250, validation_split=0.1, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wgrn6iFxxu52",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "692da22d-77f7-4a33-96c8-30ad9e2ad6c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "# Save the trained model\n",
        "model.save(\"/content/content/my_best_model_liquid.hdf5\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0watI-LtQxQr"
      },
      "outputs": [],
      "source": [
        "from ncps.tf import LTC, LTCCell  # Make sure you import both LTC and LTCCell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lY9oHcCewLta",
        "outputId": "1d61d503-bcf6-4c1b-c92d-e487a0616fd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " ltc (LTC)                   (None, 1)                 19924     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 19924 (77.83 KB)\n",
            "Trainable params: 19924 (77.83 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Recreate the wiring structure exactly as it was during training\n",
        "fc_wiring = wirings.FullyConnected(units, 1)  #1 output neuron (same wiring as during training)\n",
        "\n",
        "# Model definition using only the LTC architecture (no additional dense layers)\n",
        "model1 = keras.models.Sequential(\n",
        "    [\n",
        "        keras.layers.InputLayer(input_shape=(sequence_length, 121)),\n",
        "        LTC(fc_wiring, return_sequences=False),  # LTC Layer (returning only the last output)\n",
        "        # keras.layers.Dense(64, activation=\"relu\"),  # Dense layer with 64 units\n",
        "        # keras.layers.Dense(1, activation=\"linear\")  # Final output layer (predicting capacity)\n",
        "    ]\n",
        "\n",
        ")\n",
        "\n",
        "# Compile the model with R-squared as the key metric\n",
        "model1.compile(optimizer=keras.optimizers.Adam(0.001), loss='mean_squared_error', metrics=[RSquare()])\n",
        "\n",
        "# Step 4: Display the model summary to verify the architecture\n",
        "model1.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UA7Xm-gtSZPz"
      },
      "outputs": [],
      "source": [
        "# Step 5: Load the trained weights from your saved model file\n",
        "model1.load_weights(\"/content/Neural ODE + LTC .hdf5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BrBEYPPxSqXy",
        "outputId": "d47e816c-54fa-4c38-9b3f-fa67ec830d4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R-squared for 25C03: 0.8486119009931611\n",
            "R-squared for 45C01: 0.9957811638649253\n",
            "R-squared for 25C01: 0.9937464434814521\n",
            "R-squared for 25C04: -6.987134043466714\n",
            "R-squared for 25C07: 0.9640858227352456\n",
            "R-squared for 35C01: 0.9973647797751999\n",
            "R-squared for 25C05: 0.997959188275109\n",
            "R-squared for 25C02: 0.9623289668388555\n",
            "R-squared for 25C08: 0.5428551024065432\n",
            "R-squared for 25C06: 0.8847557171488794\n",
            "R-squared for 35C02: 0.949461517729117\n",
            "R-squared for 45C02: 0.8846603395558577\n"
          ]
        }
      ],
      "source": [
        "# List of battery files\n",
        "a = glob.glob(\"*.xlsx\")\n",
        "\n",
        "# Loop through each battery file\n",
        "for m in a:\n",
        "    # Load the battery data\n",
        "    df10 = pd.read_excel(m)\n",
        "    df101 = df10.drop(columns=['label', 'cycle number', 'Unnamed: 0', \"Capacity/mA.h\"])\n",
        "\n",
        "    # Initialize with the first 'sequence_length' capacity values for prediction\n",
        "    true_capacity = list(df10[\"Capacity/mA.h\"])  # True capacity values for R-squared calculation\n",
        "    predicted_capacity = true_capacity[:sequence_length]  # First 'sequence_length' true values to start the prediction\n",
        "\n",
        "    # Iterating through the battery data for predictions with adjustable sequence length\n",
        "    for i in range(len(df101) - (sequence_length - 1)):\n",
        "        # Create a sequence of 'sequence_length' steps (the model expects input of shape (sequence_length, 121))\n",
        "        seq = [list(df101.iloc[i + n]) for n in range(sequence_length)]\n",
        "\n",
        "        # Insert the true capacity values into the sequence at the appropriate places\n",
        "        for n in range(sequence_length):\n",
        "            seq[n].insert(0, predicted_capacity[i + n])  # Insert the corresponding capacity value\n",
        "\n",
        "        # Convert the sequence into a numpy array and reshape it to (1, sequence_length, 121)\n",
        "        seq = np.array(seq).reshape(1, sequence_length, 121)\n",
        "\n",
        "        # Make the prediction using the Liquid Neural Network model\n",
        "        predict = model1(tf.convert_to_tensor(seq, dtype=tf.float32))\n",
        "\n",
        "        # Append the predicted capacity value\n",
        "        predicted_capacity.append(predict.numpy()[0][0])\n",
        "\n",
        "    # Since predicted_capacity has extra initial values, slice it to match true_capacity length\n",
        "    predicted_capacity = predicted_capacity[:len(true_capacity)]\n",
        "\n",
        "    # Compute R-squared value for the predictions\n",
        "    r_squared = r2_score(true_capacity, predicted_capacity)\n",
        "    print(f\"R-squared for {m.split('.')[0]}: {r_squared}\")\n",
        "\n",
        "    # Save the predicted results for this battery\n",
        "    results = pd.DataFrame({'True Capacity': true_capacity, 'Predicted Capacity': predicted_capacity})\n",
        "    with pd.ExcelWriter(\"/content/content/\" + m.split('.')[0] + \"_results_with_prediction.xlsx\") as writer:\n",
        "        results.to_excel(writer, index=False)\n",
        "\n",
        "    # Plot the true and predicted capacity values for this battery\n",
        "    plt.figure()  # Create a new figure for each battery\n",
        "    plt.plot(true_capacity, label='True Capacity', color='blue')\n",
        "    plt.plot(predicted_capacity, label='Predicted Capacity', color='red')\n",
        "    plt.title(f\"True vs Predicted Capacity over Cycles for {m.split('.')[0]}\")\n",
        "    plt.xlabel(\"Cycle Number\")\n",
        "    plt.ylabel(\"Capacity (mA.h)\")\n",
        "    plt.legend()\n",
        "    plt.savefig(f\"{m.split('.')[0]}_Capacity_True_vs_Predicted.png\")  # Save the plot as a PNG file\n",
        "    plt.close()  # Close the plot to avoid overlap with the next one"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4FnC6k2_XQ8L"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}