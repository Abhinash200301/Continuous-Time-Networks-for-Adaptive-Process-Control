{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abhinash200301/Continuous-Time-Networks-for-Adaptive-Process-Control/blob/main/NeuralODE_%2B_RNN_fullyConnected.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79soaii8iRvz"
      },
      "source": [
        "# Imports and Setups"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ylFg6ScZhtsC",
        "outputId": "d0b0b234-4d4c-4a2f-f748-7ce40bd54817"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow==2.13.0 in /usr/local/lib/python3.10/dist-packages (2.13.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.1.21 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (24.3.25)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (1.64.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (3.12.1)\n",
            "Requirement already satisfied: keras<2.14,>=2.13.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (2.13.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (18.1.1)\n",
            "Requirement already satisfied: numpy<=1.24.3,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (1.24.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.14,>=2.13 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (2.13.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (2.13.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (4.5.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (1.16.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.13.0) (0.44.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0) (3.7)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0) (3.0.6)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (3.2.2)\n",
            "Requirement already satisfied: keras==2.13.1 in /usr/local/lib/python3.10/dist-packages (2.13.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow==2.13.0\n",
        "!pip install keras==2.13.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KcF4Y_zFGKEa",
        "outputId": "495c243b-72d6-479d-b96a-f783a517e62d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ncps\n",
            "  Downloading ncps-1.0.1-py3-none-any.whl.metadata (702 bytes)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from ncps) (1.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from ncps) (24.1)\n",
            "Downloading ncps-1.0.1-py3-none-any.whl (60 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/60.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.3/60.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ncps\n",
            "Successfully installed ncps-1.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install ncps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RjEkJt145zUk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from tensorflow.keras import layers, models, regularizers\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import r2_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxxU8Dwb9qeA",
        "outputId": "c69f5aa0-34df-40f8-a5d6-9ee868af5a01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow_addons\n",
            "  Downloading tensorflow_addons-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow_addons) (24.1)\n",
            "Collecting typeguard<3.0.0,>=2.7 (from tensorflow_addons)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl.metadata (3.6 kB)\n",
            "Downloading tensorflow_addons-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (611 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.8/611.8 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: typeguard, tensorflow_addons\n",
            "  Attempting uninstall: typeguard\n",
            "    Found existing installation: typeguard 4.4.0\n",
            "    Uninstalling typeguard-4.4.0:\n",
            "      Successfully uninstalled typeguard-4.4.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "inflect 7.4.0 requires typeguard>=4.0.1, but you have typeguard 2.13.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tensorflow_addons-0.23.0 typeguard-2.13.3\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow_addons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NplcANUUGJKV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6deea010-3a3b-42d6-9488-fd1b0ff3358f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from ncps import wirings\n",
        "from ncps.tf import LTC, LTCCell\n",
        "import seaborn as sns\n",
        "from tensorflow_addons.metrics import RSquare"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hX09L5T4lh93"
      },
      "outputs": [],
      "source": [
        "import tensorflow_addons as tfa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JcHei9s3XB-t"
      },
      "outputs": [],
      "source": [
        "df= pd.read_csv(\"/content/PostDischarge_Data.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lr2BHEG0cfOv"
      },
      "outputs": [],
      "source": [
        "label=df[\"label\"]\n",
        "ct=[]\n",
        "for i in np.unique(label):\n",
        "  count=0\n",
        "  for j in range(len(df)):\n",
        "    if df[\"label\"][j]== i:\n",
        "      count=count+1\n",
        "  ct.append(count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9CcdXecU4k_h"
      },
      "outputs": [],
      "source": [
        "m=np.unique(label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hBnutLIBFEG",
        "outputId": "2dd4a116-3e92-4d84-fb12-18dd37a606d2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[349, 180, 201, 34, 328, 212, 140, 36, 325, 317, 298, 309]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "ct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cE4exGOQ_zyB",
        "outputId": "d00f1c70-10d7-47ba-c537-3d7d7919d9ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "349\n",
            "529\n",
            "730\n",
            "764\n",
            "1092\n",
            "1304\n",
            "1444\n",
            "1480\n",
            "1805\n",
            "2122\n",
            "2420\n",
            "2729\n"
          ]
        }
      ],
      "source": [
        "l=0\n",
        "for i in range(len(ct)):\n",
        "  df1= df[l:l+ct[i]]\n",
        "  with pd.ExcelWriter(m[i]+\".xlsx\") as writer:\n",
        "    df1.to_excel(writer)\n",
        "  l= ct[i]+l\n",
        "  print(l)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZkbSw1_J6doA"
      },
      "outputs": [],
      "source": [
        "sequence_length = 5  # Length of the input sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k1KzHI_oDfrg"
      },
      "outputs": [],
      "source": [
        "# Get the list of Excel files\n",
        "a = glob.glob(\"*.xlsx\")\n",
        "\n",
        "# Lists to store inputs and outputs\n",
        "train_input = []\n",
        "train_output = []\n",
        "\n",
        "# Loop through each file\n",
        "for j in a:\n",
        "    dfk = pd.read_excel(j)\n",
        "    dfm = dfk.drop(columns=['label', 'cycle number', 'Unnamed: 0'])  # Drop unnecessary columns\n",
        "\n",
        "    # Loop to create sequences based on the specified sequence length\n",
        "    for i in range(len(dfk) - sequence_length):\n",
        "        ink = []\n",
        "        # Collecting the previous 'sequence_length' steps\n",
        "        for n in range(sequence_length):\n",
        "            ink.append(list(dfm[i + n:i + n + 1].values[0]))\n",
        "        train_input.append(ink)\n",
        "\n",
        "        # Predict the next value after the sequence\n",
        "        train_output.append(list(dfm[i + sequence_length:i + sequence_length + 1].values[0])[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OVtfZgrK6jeW"
      },
      "outputs": [],
      "source": [
        "# Convert lists to numpy arrays\n",
        "train_input = np.array(train_input)\n",
        "train_output = np.array(train_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zeqx6R6X9XGQ"
      },
      "outputs": [],
      "source": [
        "# Ensure train_output is reshaped to (samples, 1)\n",
        "train_output = train_output.reshape(-1, 1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "units = 32  # Number of units"
      ],
      "metadata": {
        "id": "J6NzKoEJCn4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dz99h2gAqW8w"
      },
      "outputs": [],
      "source": [
        "# # Wiring: with 1 motor neuron for output\n",
        "# fc_wiring = wirings.FullyConnected(units, 1)  # 1 output neuron"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0aouUFXSqW5T"
      },
      "outputs": [],
      "source": [
        "model = keras.models.Sequential(\n",
        "    [\n",
        "        keras.layers.InputLayer(input_shape=(sequence_length, 121)),\n",
        "        keras.layers.SimpleRNN(units, return_sequences=False),  # Simple RNN layer with 64 units\n",
        "        keras.layers.Dense(32, activation=\"relu\"),  # Dense layer with 64 units\n",
        "        keras.layers.Dense(1, activation=\"linear\")  # Final output layer (predicting capacity)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Compile the model with R-squared as the key metric\n",
        "model.compile(optimizer=keras.optimizers.Adam(0.001), loss='mean_squared_error', metrics=[RSquare()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6YMVvu1sm1Ul",
        "outputId": "47f33de1-57d9-418a-8070-048af3869bed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " simple_rnn_2 (SimpleRNN)    (None, 32)                4928      \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 32)                1056      \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 6017 (23.50 KB)\n",
            "Trainable params: 6017 (23.50 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Clearing any previous session\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# # Building and training the new model with Liquid Neural Network\n",
        "# model = get_model()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HBhpzX0jK7pX"
      },
      "outputs": [],
      "source": [
        "# # Set a larger figure size\n",
        "# sns.set_style(\"white\")\n",
        "# plt.figure(figsize=(50, 50))  # Adjust the figure size\n",
        "\n",
        "# # Draw the wiring with labels\n",
        "# legend_handles = fc_wiring.draw_graph(draw_labels=True)\n",
        "\n",
        "# # Add a legend\n",
        "# plt.legend(handles=legend_handles, loc=\"upper right\", bbox_to_anchor=(1.3, 1))\n",
        "\n",
        "# # Despine and adjust layout\n",
        "# sns.despine(left=True, bottom=True)\n",
        "# plt.tight_layout()\n",
        "\n",
        "# # Show the plot\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MAIkcxkpvlP"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GvUujlDKm1Tu",
        "outputId": "91267828-959e-4b14-efeb-9a95bacd5722"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/250\n",
            "76/76 [==============================] - 2s 8ms/step - loss: 598.1843 - r_square: -18.5210 - val_loss: 304.0209 - val_r_square: -58.8315\n",
            "Epoch 2/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 129.4601 - r_square: -3.2247 - val_loss: 25.0120 - val_r_square: -3.9224\n",
            "Epoch 3/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 14.6127 - r_square: 0.5231 - val_loss: 3.0216 - val_r_square: 0.4053\n",
            "Epoch 4/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 5.5142 - r_square: 0.8200 - val_loss: 1.0653 - val_r_square: 0.7903\n",
            "Epoch 5/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 2.4979 - r_square: 0.9185 - val_loss: 0.6884 - val_r_square: 0.8645\n",
            "Epoch 6/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 1.3550 - r_square: 0.9558 - val_loss: 0.6133 - val_r_square: 0.8793\n",
            "Epoch 7/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.7633 - r_square: 0.9751 - val_loss: 0.5165 - val_r_square: 0.8984\n",
            "Epoch 8/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.5199 - r_square: 0.9830 - val_loss: 0.5623 - val_r_square: 0.8893\n",
            "Epoch 9/250\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 0.3921 - r_square: 0.9872 - val_loss: 0.5102 - val_r_square: 0.8996\n",
            "Epoch 10/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.3327 - r_square: 0.9891 - val_loss: 0.5012 - val_r_square: 0.9014\n",
            "Epoch 11/250\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 0.2899 - r_square: 0.9905 - val_loss: 0.5904 - val_r_square: 0.8838\n",
            "Epoch 12/250\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 0.2573 - r_square: 0.9916 - val_loss: 0.7429 - val_r_square: 0.8538\n",
            "Epoch 13/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.2552 - r_square: 0.9917 - val_loss: 0.5148 - val_r_square: 0.8987\n",
            "Epoch 14/250\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 0.2246 - r_square: 0.9927 - val_loss: 0.6080 - val_r_square: 0.8804\n",
            "Epoch 15/250\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 0.2464 - r_square: 0.9920 - val_loss: 0.5115 - val_r_square: 0.8993\n",
            "Epoch 16/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.2138 - r_square: 0.9930 - val_loss: 0.5263 - val_r_square: 0.8964\n",
            "Epoch 17/250\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 0.2255 - r_square: 0.9926 - val_loss: 0.5075 - val_r_square: 0.9001\n",
            "Epoch 18/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.2205 - r_square: 0.9928 - val_loss: 0.5836 - val_r_square: 0.8851\n",
            "Epoch 19/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.2153 - r_square: 0.9930 - val_loss: 0.5114 - val_r_square: 0.8993\n",
            "Epoch 20/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.2160 - r_square: 0.9930 - val_loss: 0.5547 - val_r_square: 0.8908\n",
            "Epoch 21/250\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.2076 - r_square: 0.9932 - val_loss: 0.5981 - val_r_square: 0.8823\n",
            "Epoch 22/250\n",
            "76/76 [==============================] - 1s 12ms/step - loss: 0.2057 - r_square: 0.9933 - val_loss: 0.6091 - val_r_square: 0.8801\n",
            "Epoch 23/250\n",
            "76/76 [==============================] - 1s 8ms/step - loss: 0.2026 - r_square: 0.9934 - val_loss: 0.5273 - val_r_square: 0.8962\n",
            "Epoch 24/250\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.2136 - r_square: 0.9930 - val_loss: 0.5261 - val_r_square: 0.8965\n",
            "Epoch 25/250\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1998 - r_square: 0.9935 - val_loss: 0.5275 - val_r_square: 0.8962\n",
            "Epoch 26/250\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.2221 - r_square: 0.9928 - val_loss: 0.5161 - val_r_square: 0.8984\n",
            "Epoch 27/250\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.2088 - r_square: 0.9932 - val_loss: 0.5820 - val_r_square: 0.8855\n",
            "Epoch 28/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.2266 - r_square: 0.9926 - val_loss: 0.5107 - val_r_square: 0.8995\n",
            "Epoch 29/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.1991 - r_square: 0.9935 - val_loss: 0.6188 - val_r_square: 0.8782\n",
            "Epoch 30/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.1864 - r_square: 0.9939 - val_loss: 0.5214 - val_r_square: 0.8974\n",
            "Epoch 31/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.1904 - r_square: 0.9938 - val_loss: 0.6014 - val_r_square: 0.8816\n",
            "Epoch 32/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.1900 - r_square: 0.9938 - val_loss: 0.5195 - val_r_square: 0.8978\n",
            "Epoch 33/250\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 0.1894 - r_square: 0.9938 - val_loss: 0.5278 - val_r_square: 0.8961\n",
            "Epoch 34/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.2000 - r_square: 0.9935 - val_loss: 0.5241 - val_r_square: 0.8969\n",
            "Epoch 35/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.1944 - r_square: 0.9937 - val_loss: 0.5145 - val_r_square: 0.8987\n",
            "Epoch 36/250\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 0.1926 - r_square: 0.9937 - val_loss: 0.5149 - val_r_square: 0.8987\n",
            "Epoch 37/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.1902 - r_square: 0.9938 - val_loss: 0.5607 - val_r_square: 0.8897\n",
            "Epoch 38/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.1881 - r_square: 0.9939 - val_loss: 0.5656 - val_r_square: 0.8887\n",
            "Epoch 39/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.1903 - r_square: 0.9938 - val_loss: 0.5494 - val_r_square: 0.8919\n",
            "Epoch 40/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.1920 - r_square: 0.9937 - val_loss: 0.5165 - val_r_square: 0.8983\n",
            "Epoch 41/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.1959 - r_square: 0.9936 - val_loss: 0.5395 - val_r_square: 0.8938\n",
            "Epoch 42/250\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 0.2108 - r_square: 0.9931 - val_loss: 0.5435 - val_r_square: 0.8930\n",
            "Epoch 43/250\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 0.2003 - r_square: 0.9935 - val_loss: 0.5257 - val_r_square: 0.8965\n",
            "Epoch 44/250\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 0.2015 - r_square: 0.9934 - val_loss: 0.5200 - val_r_square: 0.8977\n",
            "Epoch 45/250\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 0.1896 - r_square: 0.9938 - val_loss: 0.5284 - val_r_square: 0.8960\n",
            "Epoch 46/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.1853 - r_square: 0.9940 - val_loss: 0.5186 - val_r_square: 0.8979\n",
            "Epoch 47/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.1948 - r_square: 0.9936 - val_loss: 0.5976 - val_r_square: 0.8824\n",
            "Epoch 48/250\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 0.1832 - r_square: 0.9940 - val_loss: 0.5384 - val_r_square: 0.8940\n",
            "Epoch 49/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.1905 - r_square: 0.9938 - val_loss: 0.5997 - val_r_square: 0.8820\n",
            "Epoch 50/250\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 0.1817 - r_square: 0.9941 - val_loss: 0.5698 - val_r_square: 0.8879\n",
            "Epoch 51/250\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1826 - r_square: 0.9940 - val_loss: 0.5763 - val_r_square: 0.8866\n",
            "Epoch 52/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.1927 - r_square: 0.9937 - val_loss: 0.5301 - val_r_square: 0.8957\n",
            "Epoch 53/250\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 0.1838 - r_square: 0.9940 - val_loss: 0.5545 - val_r_square: 0.8909\n",
            "Epoch 54/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.2040 - r_square: 0.9933 - val_loss: 0.5617 - val_r_square: 0.8895\n",
            "Epoch 55/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.1899 - r_square: 0.9938 - val_loss: 0.5189 - val_r_square: 0.8979\n",
            "Epoch 56/250\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.2018 - r_square: 0.9934 - val_loss: 0.5201 - val_r_square: 0.8977\n",
            "Epoch 57/250\n",
            "76/76 [==============================] - 0s 7ms/step - loss: 0.1973 - r_square: 0.9936 - val_loss: 0.5379 - val_r_square: 0.8941\n",
            "Epoch 58/250\n",
            "76/76 [==============================] - 1s 8ms/step - loss: 0.1849 - r_square: 0.9940 - val_loss: 0.5429 - val_r_square: 0.8932\n",
            "Epoch 59/250\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.2060 - r_square: 0.9933 - val_loss: 0.5130 - val_r_square: 0.8990\n",
            "Epoch 60/250\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1874 - r_square: 0.9939 - val_loss: 0.5174 - val_r_square: 0.8982\n",
            "Epoch 61/250\n",
            "76/76 [==============================] - 1s 8ms/step - loss: 0.1919 - r_square: 0.9937 - val_loss: 0.5532 - val_r_square: 0.8911\n",
            "Epoch 62/250\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.2275 - r_square: 0.9926 - val_loss: 0.5218 - val_r_square: 0.8973\n",
            "Epoch 63/250\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1786 - r_square: 0.9942 - val_loss: 0.5215 - val_r_square: 0.8974\n",
            "Epoch 64/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.1861 - r_square: 0.9939 - val_loss: 0.5212 - val_r_square: 0.8974\n",
            "Epoch 65/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.1985 - r_square: 0.9935 - val_loss: 0.5405 - val_r_square: 0.8936\n",
            "Epoch 66/250\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 0.1895 - r_square: 0.9938 - val_loss: 0.5506 - val_r_square: 0.8916\n",
            "Epoch 67/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.1947 - r_square: 0.9936 - val_loss: 0.5308 - val_r_square: 0.8955\n",
            "Epoch 68/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.1840 - r_square: 0.9940 - val_loss: 0.5525 - val_r_square: 0.8913\n",
            "Epoch 69/250\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 0.1963 - r_square: 0.9936 - val_loss: 0.5133 - val_r_square: 0.8990\n",
            "Epoch 70/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.2055 - r_square: 0.9933 - val_loss: 0.5906 - val_r_square: 0.8838\n",
            "Epoch 71/250\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 0.1889 - r_square: 0.9938 - val_loss: 0.5415 - val_r_square: 0.8934\n",
            "Epoch 72/250\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 0.1898 - r_square: 0.9938 - val_loss: 0.5841 - val_r_square: 0.8850\n",
            "Epoch 73/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.1849 - r_square: 0.9940 - val_loss: 0.5936 - val_r_square: 0.8832\n",
            "Epoch 74/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.2123 - r_square: 0.9931 - val_loss: 0.5272 - val_r_square: 0.8962\n",
            "Epoch 75/250\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1932 - r_square: 0.9937 - val_loss: 0.8678 - val_r_square: 0.8292\n",
            "Epoch 76/250\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 0.3559 - r_square: 0.9884 - val_loss: 0.5174 - val_r_square: 0.8982\n",
            "Epoch 77/250\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1868 - r_square: 0.9939 - val_loss: 0.5307 - val_r_square: 0.8956\n",
            "Epoch 78/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.1869 - r_square: 0.9939 - val_loss: 0.5587 - val_r_square: 0.8901\n",
            "Epoch 79/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.1838 - r_square: 0.9940 - val_loss: 0.5287 - val_r_square: 0.8960\n",
            "Epoch 80/250\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 0.1796 - r_square: 0.9941 - val_loss: 0.5185 - val_r_square: 0.8980\n",
            "Epoch 81/250\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.2025 - r_square: 0.9934 - val_loss: 0.5555 - val_r_square: 0.8907\n",
            "Epoch 82/250\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1922 - r_square: 0.9937 - val_loss: 0.5527 - val_r_square: 0.8912\n",
            "Epoch 83/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.1820 - r_square: 0.9941 - val_loss: 0.5511 - val_r_square: 0.8915\n",
            "Epoch 84/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.1828 - r_square: 0.9940 - val_loss: 0.5378 - val_r_square: 0.8942\n",
            "Epoch 85/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.1809 - r_square: 0.9941 - val_loss: 0.5155 - val_r_square: 0.8985\n",
            "Epoch 86/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.1820 - r_square: 0.9941 - val_loss: 0.5613 - val_r_square: 0.8895\n",
            "Epoch 87/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.1826 - r_square: 0.9940 - val_loss: 0.5163 - val_r_square: 0.8984\n",
            "Epoch 88/250\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 0.1892 - r_square: 0.9938 - val_loss: 0.5257 - val_r_square: 0.8965\n",
            "Epoch 89/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.2108 - r_square: 0.9931 - val_loss: 0.5144 - val_r_square: 0.8988\n",
            "Epoch 90/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.2137 - r_square: 0.9930 - val_loss: 0.6109 - val_r_square: 0.8798\n",
            "Epoch 91/250\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1891 - r_square: 0.9938 - val_loss: 0.6195 - val_r_square: 0.8781\n",
            "Epoch 92/250\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1975 - r_square: 0.9936 - val_loss: 0.5137 - val_r_square: 0.8989\n",
            "Epoch 93/250\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1812 - r_square: 0.9941 - val_loss: 0.5153 - val_r_square: 0.8986\n",
            "Epoch 94/250\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1896 - r_square: 0.9938 - val_loss: 0.5568 - val_r_square: 0.8904\n",
            "Epoch 95/250\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1863 - r_square: 0.9939 - val_loss: 0.5206 - val_r_square: 0.8975\n",
            "Epoch 96/250\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1715 - r_square: 0.9944 - val_loss: 0.5194 - val_r_square: 0.8978\n",
            "Epoch 97/250\n",
            "76/76 [==============================] - 1s 8ms/step - loss: 0.1818 - r_square: 0.9941 - val_loss: 0.5249 - val_r_square: 0.8967\n",
            "Epoch 98/250\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 0.1868 - r_square: 0.9939 - val_loss: 0.5168 - val_r_square: 0.8983\n",
            "Epoch 99/250\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 0.1916 - r_square: 0.9937 - val_loss: 0.5265 - val_r_square: 0.8964\n",
            "Epoch 100/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.1839 - r_square: 0.9940 - val_loss: 0.5250 - val_r_square: 0.8967\n",
            "Epoch 101/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.1886 - r_square: 0.9938 - val_loss: 0.5320 - val_r_square: 0.8953\n",
            "Epoch 102/250\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 0.1802 - r_square: 0.9941 - val_loss: 0.5341 - val_r_square: 0.8949\n",
            "Epoch 103/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.1895 - r_square: 0.9938 - val_loss: 0.5893 - val_r_square: 0.8840\n",
            "Epoch 104/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.1765 - r_square: 0.9942 - val_loss: 0.5764 - val_r_square: 0.8866\n",
            "Epoch 105/250\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 0.1895 - r_square: 0.9938 - val_loss: 0.6028 - val_r_square: 0.8814\n",
            "Epoch 106/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.1873 - r_square: 0.9939 - val_loss: 0.5252 - val_r_square: 0.8966\n",
            "Epoch 107/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.2021 - r_square: 0.9934 - val_loss: 0.5362 - val_r_square: 0.8945\n",
            "Epoch 108/250\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 0.2191 - r_square: 0.9929 - val_loss: 0.5218 - val_r_square: 0.8973\n",
            "Epoch 109/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.2012 - r_square: 0.9934 - val_loss: 0.5224 - val_r_square: 0.8972\n",
            "Epoch 110/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.1919 - r_square: 0.9937 - val_loss: 0.5305 - val_r_square: 0.8956\n",
            "Epoch 111/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.1879 - r_square: 0.9939 - val_loss: 0.5114 - val_r_square: 0.8994\n",
            "Epoch 112/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.1932 - r_square: 0.9937 - val_loss: 0.5559 - val_r_square: 0.8906\n",
            "Epoch 113/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.1877 - r_square: 0.9939 - val_loss: 0.7921 - val_r_square: 0.8441\n",
            "Epoch 114/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.2478 - r_square: 0.9919 - val_loss: 0.5160 - val_r_square: 0.8985\n",
            "Epoch 115/250\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 0.1922 - r_square: 0.9937 - val_loss: 0.5137 - val_r_square: 0.8989\n",
            "Epoch 116/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.1851 - r_square: 0.9940 - val_loss: 0.5207 - val_r_square: 0.8975\n",
            "Epoch 117/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.1997 - r_square: 0.9935 - val_loss: 0.5508 - val_r_square: 0.8916\n",
            "Epoch 118/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.1851 - r_square: 0.9940 - val_loss: 0.5481 - val_r_square: 0.8921\n",
            "Epoch 119/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.1971 - r_square: 0.9936 - val_loss: 0.5341 - val_r_square: 0.8949\n",
            "Epoch 120/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.1781 - r_square: 0.9942 - val_loss: 0.5216 - val_r_square: 0.8974\n",
            "Epoch 121/250\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 0.1748 - r_square: 0.9943 - val_loss: 0.5221 - val_r_square: 0.8973\n",
            "Epoch 122/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.1753 - r_square: 0.9943 - val_loss: 0.5110 - val_r_square: 0.8994\n",
            "Epoch 123/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.1817 - r_square: 0.9941 - val_loss: 0.5377 - val_r_square: 0.8942\n",
            "Epoch 124/250\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 0.1872 - r_square: 0.9939 - val_loss: 0.5218 - val_r_square: 0.8973\n",
            "Epoch 125/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.1872 - r_square: 0.9939 - val_loss: 0.5735 - val_r_square: 0.8871\n",
            "Epoch 126/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.2297 - r_square: 0.9925 - val_loss: 0.5158 - val_r_square: 0.8985\n",
            "Epoch 127/250\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 0.2468 - r_square: 0.9919 - val_loss: 0.5390 - val_r_square: 0.8939\n",
            "Epoch 128/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.1868 - r_square: 0.9939 - val_loss: 0.5180 - val_r_square: 0.8981\n",
            "Epoch 129/250\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 0.1869 - r_square: 0.9939 - val_loss: 0.5199 - val_r_square: 0.8977\n",
            "Epoch 130/250\n",
            "76/76 [==============================] - 1s 8ms/step - loss: 0.1681 - r_square: 0.9945 - val_loss: 0.5349 - val_r_square: 0.8947\n",
            "Epoch 131/250\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1986 - r_square: 0.9935 - val_loss: 0.5151 - val_r_square: 0.8986\n",
            "Epoch 132/250\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1820 - r_square: 0.9941 - val_loss: 0.5668 - val_r_square: 0.8884\n",
            "Epoch 133/250\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1830 - r_square: 0.9940 - val_loss: 0.5152 - val_r_square: 0.8986\n",
            "Epoch 134/250\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1857 - r_square: 0.9939 - val_loss: 0.5561 - val_r_square: 0.8906\n",
            "Epoch 135/250\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1847 - r_square: 0.9940 - val_loss: 0.5657 - val_r_square: 0.8887\n",
            "Epoch 136/250\n",
            "76/76 [==============================] - 1s 8ms/step - loss: 0.1725 - r_square: 0.9944 - val_loss: 0.5128 - val_r_square: 0.8991\n",
            "Epoch 137/250\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 0.1841 - r_square: 0.9940 - val_loss: 0.5300 - val_r_square: 0.8957\n",
            "Epoch 138/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.1853 - r_square: 0.9940 - val_loss: 0.5515 - val_r_square: 0.8915\n",
            "Epoch 139/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.1912 - r_square: 0.9938 - val_loss: 0.5254 - val_r_square: 0.8966\n",
            "Epoch 140/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.1747 - r_square: 0.9943 - val_loss: 0.5307 - val_r_square: 0.8955\n",
            "Epoch 141/250\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 0.1909 - r_square: 0.9938 - val_loss: 0.5313 - val_r_square: 0.8954\n",
            "Epoch 142/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.1801 - r_square: 0.9941 - val_loss: 0.5258 - val_r_square: 0.8965\n",
            "Epoch 143/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.1907 - r_square: 0.9938 - val_loss: 0.5255 - val_r_square: 0.8966\n",
            "Epoch 144/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.1763 - r_square: 0.9942 - val_loss: 0.5162 - val_r_square: 0.8984\n",
            "Epoch 145/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.2081 - r_square: 0.9932 - val_loss: 0.5372 - val_r_square: 0.8943\n",
            "Epoch 146/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.1701 - r_square: 0.9944 - val_loss: 0.7685 - val_r_square: 0.8488\n",
            "Epoch 147/250\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 0.2021 - r_square: 0.9934 - val_loss: 0.5185 - val_r_square: 0.8980\n",
            "Epoch 148/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.1761 - r_square: 0.9943 - val_loss: 0.5291 - val_r_square: 0.8959\n",
            "Epoch 149/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.1874 - r_square: 0.9939 - val_loss: 0.5223 - val_r_square: 0.8972\n",
            "Epoch 150/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.1788 - r_square: 0.9942 - val_loss: 0.5174 - val_r_square: 0.8982\n",
            "Epoch 151/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.1708 - r_square: 0.9944 - val_loss: 0.6157 - val_r_square: 0.8788\n",
            "Epoch 152/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.1945 - r_square: 0.9937 - val_loss: 0.5621 - val_r_square: 0.8894\n",
            "Epoch 153/250\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 0.1703 - r_square: 0.9944 - val_loss: 0.5210 - val_r_square: 0.8975\n",
            "Epoch 154/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.1741 - r_square: 0.9943 - val_loss: 0.5184 - val_r_square: 0.8980\n",
            "Epoch 155/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.1725 - r_square: 0.9944 - val_loss: 0.5310 - val_r_square: 0.8955\n",
            "Epoch 156/250\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 0.1799 - r_square: 0.9941 - val_loss: 1.0152 - val_r_square: 0.8002\n",
            "Epoch 157/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.2380 - r_square: 0.9922 - val_loss: 0.5243 - val_r_square: 0.8968\n",
            "Epoch 158/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.1962 - r_square: 0.9936 - val_loss: 0.5380 - val_r_square: 0.8941\n",
            "Epoch 159/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.1738 - r_square: 0.9943 - val_loss: 0.5244 - val_r_square: 0.8968\n",
            "Epoch 160/250\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 0.1792 - r_square: 0.9942 - val_loss: 0.5699 - val_r_square: 0.8878\n",
            "Epoch 161/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.1879 - r_square: 0.9939 - val_loss: 0.5146 - val_r_square: 0.8987\n",
            "Epoch 162/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.1924 - r_square: 0.9937 - val_loss: 0.5215 - val_r_square: 0.8974\n",
            "Epoch 163/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.1845 - r_square: 0.9940 - val_loss: 0.6177 - val_r_square: 0.8784\n",
            "Epoch 164/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.1925 - r_square: 0.9937 - val_loss: 0.5131 - val_r_square: 0.8990\n",
            "Epoch 165/250\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 0.1871 - r_square: 0.9939 - val_loss: 0.5491 - val_r_square: 0.8919\n",
            "Epoch 166/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.2018 - r_square: 0.9934 - val_loss: 0.5278 - val_r_square: 0.8961\n",
            "Epoch 167/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.1875 - r_square: 0.9939 - val_loss: 0.5231 - val_r_square: 0.8971\n",
            "Epoch 168/250\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1817 - r_square: 0.9941 - val_loss: 0.6550 - val_r_square: 0.8711\n",
            "Epoch 169/250\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1866 - r_square: 0.9939 - val_loss: 0.5482 - val_r_square: 0.8921\n",
            "Epoch 170/250\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1773 - r_square: 0.9942 - val_loss: 0.5186 - val_r_square: 0.8979\n",
            "Epoch 171/250\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1940 - r_square: 0.9937 - val_loss: 0.5210 - val_r_square: 0.8975\n",
            "Epoch 172/250\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1709 - r_square: 0.9944 - val_loss: 0.5411 - val_r_square: 0.8935\n",
            "Epoch 173/250\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1779 - r_square: 0.9942 - val_loss: 0.5461 - val_r_square: 0.8925\n",
            "Epoch 174/250\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1771 - r_square: 0.9942 - val_loss: 0.5978 - val_r_square: 0.8824\n",
            "Epoch 175/250\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1833 - r_square: 0.9940 - val_loss: 0.5408 - val_r_square: 0.8936\n",
            "Epoch 176/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.1705 - r_square: 0.9944 - val_loss: 0.6020 - val_r_square: 0.8815\n",
            "Epoch 177/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.1762 - r_square: 0.9943 - val_loss: 0.5198 - val_r_square: 0.8977\n",
            "Epoch 178/250\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 0.1935 - r_square: 0.9937 - val_loss: 0.5869 - val_r_square: 0.8845\n",
            "Epoch 179/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.1878 - r_square: 0.9939 - val_loss: 0.5529 - val_r_square: 0.8912\n",
            "Epoch 180/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.2801 - r_square: 0.9909 - val_loss: 0.5965 - val_r_square: 0.8826\n",
            "Epoch 181/250\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 0.2143 - r_square: 0.9930 - val_loss: 0.5190 - val_r_square: 0.8979\n",
            "Epoch 182/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.1837 - r_square: 0.9940 - val_loss: 0.5214 - val_r_square: 0.8974\n",
            "Epoch 183/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.1932 - r_square: 0.9937 - val_loss: 0.5232 - val_r_square: 0.8970\n",
            "Epoch 184/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.1879 - r_square: 0.9939 - val_loss: 0.5137 - val_r_square: 0.8989\n",
            "Epoch 185/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.1926 - r_square: 0.9937 - val_loss: 0.5553 - val_r_square: 0.8907\n",
            "Epoch 186/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.1916 - r_square: 0.9937 - val_loss: 0.5526 - val_r_square: 0.8913\n",
            "Epoch 187/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.1919 - r_square: 0.9937 - val_loss: 0.5368 - val_r_square: 0.8944\n",
            "Epoch 188/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.1757 - r_square: 0.9943 - val_loss: 0.6184 - val_r_square: 0.8783\n",
            "Epoch 189/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.1920 - r_square: 0.9937 - val_loss: 0.5628 - val_r_square: 0.8892\n",
            "Epoch 190/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.1942 - r_square: 0.9937 - val_loss: 0.5198 - val_r_square: 0.8977\n",
            "Epoch 191/250\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 0.2027 - r_square: 0.9934 - val_loss: 0.5957 - val_r_square: 0.8828\n",
            "Epoch 192/250\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1941 - r_square: 0.9937 - val_loss: 0.5163 - val_r_square: 0.8984\n",
            "Epoch 193/250\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1907 - r_square: 0.9938 - val_loss: 0.5203 - val_r_square: 0.8976\n",
            "Epoch 194/250\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 0.2011 - r_square: 0.9934 - val_loss: 0.5316 - val_r_square: 0.8954\n",
            "Epoch 195/250\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1943 - r_square: 0.9937 - val_loss: 0.5213 - val_r_square: 0.8974\n",
            "Epoch 196/250\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 0.1832 - r_square: 0.9940 - val_loss: 0.5430 - val_r_square: 0.8931\n",
            "Epoch 197/250\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1916 - r_square: 0.9937 - val_loss: 0.5160 - val_r_square: 0.8984\n",
            "Epoch 198/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.1912 - r_square: 0.9938 - val_loss: 0.5254 - val_r_square: 0.8966\n",
            "Epoch 199/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.1722 - r_square: 0.9944 - val_loss: 0.5354 - val_r_square: 0.8946\n",
            "Epoch 200/250\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 0.1957 - r_square: 0.9936 - val_loss: 0.5168 - val_r_square: 0.8983\n",
            "Epoch 201/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.1838 - r_square: 0.9940 - val_loss: 0.5154 - val_r_square: 0.8986\n",
            "Epoch 202/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.1866 - r_square: 0.9939 - val_loss: 0.5510 - val_r_square: 0.8916\n",
            "Epoch 203/250\n",
            "76/76 [==============================] - 1s 8ms/step - loss: 0.1840 - r_square: 0.9940 - val_loss: 0.7026 - val_r_square: 0.8617\n",
            "Epoch 204/250\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.2022 - r_square: 0.9934 - val_loss: 0.5467 - val_r_square: 0.8924\n",
            "Epoch 205/250\n",
            "76/76 [==============================] - 1s 8ms/step - loss: 0.1832 - r_square: 0.9940 - val_loss: 0.5281 - val_r_square: 0.8961\n",
            "Epoch 206/250\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1874 - r_square: 0.9939 - val_loss: 0.5335 - val_r_square: 0.8950\n",
            "Epoch 207/250\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1759 - r_square: 0.9943 - val_loss: 0.6220 - val_r_square: 0.8776\n",
            "Epoch 208/250\n",
            "76/76 [==============================] - 0s 7ms/step - loss: 0.1994 - r_square: 0.9935 - val_loss: 0.5352 - val_r_square: 0.8947\n",
            "Epoch 209/250\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1787 - r_square: 0.9942 - val_loss: 0.5732 - val_r_square: 0.8872\n",
            "Epoch 210/250\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1916 - r_square: 0.9937 - val_loss: 0.5475 - val_r_square: 0.8923\n",
            "Epoch 211/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.1804 - r_square: 0.9941 - val_loss: 0.7060 - val_r_square: 0.8611\n",
            "Epoch 212/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.1864 - r_square: 0.9939 - val_loss: 0.5334 - val_r_square: 0.8950\n",
            "Epoch 213/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.1858 - r_square: 0.9939 - val_loss: 0.5202 - val_r_square: 0.8976\n",
            "Epoch 214/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.1827 - r_square: 0.9940 - val_loss: 0.5239 - val_r_square: 0.8969\n",
            "Epoch 215/250\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 0.1986 - r_square: 0.9935 - val_loss: 0.5427 - val_r_square: 0.8932\n",
            "Epoch 216/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.1866 - r_square: 0.9939 - val_loss: 0.5336 - val_r_square: 0.8950\n",
            "Epoch 217/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.1824 - r_square: 0.9940 - val_loss: 0.5398 - val_r_square: 0.8938\n",
            "Epoch 218/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.1846 - r_square: 0.9940 - val_loss: 0.5209 - val_r_square: 0.8975\n",
            "Epoch 219/250\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 0.1848 - r_square: 0.9940 - val_loss: 0.5206 - val_r_square: 0.8975\n",
            "Epoch 220/250\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1866 - r_square: 0.9939 - val_loss: 0.5253 - val_r_square: 0.8966\n",
            "Epoch 221/250\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1884 - r_square: 0.9939 - val_loss: 0.5397 - val_r_square: 0.8938\n",
            "Epoch 222/250\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1823 - r_square: 0.9940 - val_loss: 0.5439 - val_r_square: 0.8930\n",
            "Epoch 223/250\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1937 - r_square: 0.9937 - val_loss: 0.5255 - val_r_square: 0.8966\n",
            "Epoch 224/250\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 0.1794 - r_square: 0.9941 - val_loss: 0.5243 - val_r_square: 0.8968\n",
            "Epoch 225/250\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1852 - r_square: 0.9940 - val_loss: 0.5238 - val_r_square: 0.8969\n",
            "Epoch 226/250\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1831 - r_square: 0.9940 - val_loss: 0.5827 - val_r_square: 0.8853\n",
            "Epoch 227/250\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.2340 - r_square: 0.9924 - val_loss: 0.5298 - val_r_square: 0.8957\n",
            "Epoch 228/250\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1890 - r_square: 0.9938 - val_loss: 0.5194 - val_r_square: 0.8978\n",
            "Epoch 229/250\n",
            "76/76 [==============================] - 1s 8ms/step - loss: 0.1937 - r_square: 0.9937 - val_loss: 0.5853 - val_r_square: 0.8848\n",
            "Epoch 230/250\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1983 - r_square: 0.9935 - val_loss: 0.5158 - val_r_square: 0.8985\n",
            "Epoch 231/250\n",
            "76/76 [==============================] - 1s 8ms/step - loss: 0.1865 - r_square: 0.9939 - val_loss: 0.5447 - val_r_square: 0.8928\n",
            "Epoch 232/250\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1800 - r_square: 0.9941 - val_loss: 0.5419 - val_r_square: 0.8934\n",
            "Epoch 233/250\n",
            "76/76 [==============================] - 1s 10ms/step - loss: 0.1798 - r_square: 0.9941 - val_loss: 0.5191 - val_r_square: 0.8978\n",
            "Epoch 234/250\n",
            "76/76 [==============================] - 1s 8ms/step - loss: 0.2097 - r_square: 0.9932 - val_loss: 0.5213 - val_r_square: 0.8974\n",
            "Epoch 235/250\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1918 - r_square: 0.9937 - val_loss: 0.5468 - val_r_square: 0.8924\n",
            "Epoch 236/250\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1846 - r_square: 0.9940 - val_loss: 0.5245 - val_r_square: 0.8968\n",
            "Epoch 237/250\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1754 - r_square: 0.9943 - val_loss: 0.5237 - val_r_square: 0.8969\n",
            "Epoch 238/250\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.1870 - r_square: 0.9939 - val_loss: 0.5291 - val_r_square: 0.8959\n",
            "Epoch 239/250\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.1751 - r_square: 0.9943 - val_loss: 0.5215 - val_r_square: 0.8974\n",
            "Epoch 240/250\n",
            "76/76 [==============================] - 1s 8ms/step - loss: 0.1839 - r_square: 0.9940 - val_loss: 0.5153 - val_r_square: 0.8986\n",
            "Epoch 241/250\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 0.1765 - r_square: 0.9942 - val_loss: 0.5624 - val_r_square: 0.8893\n",
            "Epoch 242/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.1994 - r_square: 0.9935 - val_loss: 0.5206 - val_r_square: 0.8975\n",
            "Epoch 243/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.1818 - r_square: 0.9941 - val_loss: 0.5200 - val_r_square: 0.8977\n",
            "Epoch 244/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.1879 - r_square: 0.9939 - val_loss: 0.5348 - val_r_square: 0.8947\n",
            "Epoch 245/250\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 0.1818 - r_square: 0.9941 - val_loss: 0.5464 - val_r_square: 0.8925\n",
            "Epoch 246/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.1791 - r_square: 0.9942 - val_loss: 0.5167 - val_r_square: 0.8983\n",
            "Epoch 247/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.1775 - r_square: 0.9942 - val_loss: 0.5267 - val_r_square: 0.8963\n",
            "Epoch 248/250\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 0.2025 - r_square: 0.9934 - val_loss: 0.5610 - val_r_square: 0.8896\n",
            "Epoch 249/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.1690 - r_square: 0.9945 - val_loss: 0.7579 - val_r_square: 0.8509\n",
            "Epoch 250/250\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.1972 - r_square: 0.9936 - val_loss: 0.5189 - val_r_square: 0.8979\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(train_input, train_output, epochs=250, validation_split=0.1, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wgrn6iFxxu52",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1dfe95ad-787f-464c-eac1-5e5e76e8593c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "# Save the trained model\n",
        "model.save(\"/content/content/my_best_model_liquid.hdf5\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0watI-LtQxQr"
      },
      "outputs": [],
      "source": [
        "# from ncps.tf import LTC, LTCCell  # Make sure you import both LTC and LTCCell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lY9oHcCewLta",
        "outputId": "7f3cabde-21e0-496e-f354-2979d268f1e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " simple_rnn (SimpleRNN)      (None, 32)                4928      \n",
            "                                                                 \n",
            " dense (Dense)               (None, 32)                1056      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 6017 (23.50 KB)\n",
            "Trainable params: 6017 (23.50 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model1 = keras.models.Sequential(\n",
        "    [\n",
        "        keras.layers.InputLayer(input_shape=(sequence_length, 121)),\n",
        "        keras.layers.SimpleRNN(units, return_sequences=False),  # Simple RNN layer with 64 units\n",
        "        keras.layers.Dense(32, activation=\"relu\"),  # Dense layer with 64 units\n",
        "        keras.layers.Dense(1, activation=\"linear\")  # Final output layer (predicting capacity)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Compile the model with R-squared as the key metric\n",
        "model1.compile(optimizer=keras.optimizers.Adam(0.001), loss='mean_squared_error', metrics=[RSquare()])\n",
        "\n",
        "model1.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UA7Xm-gtSZPz"
      },
      "outputs": [],
      "source": [
        "# Step 5: Load the trained weights from your saved model file\n",
        "model1.load_weights(\"/content/content/my_best_model_liquid.hdf5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BrBEYPPxSqXy",
        "outputId": "e16d1e40-50ab-4b64-be87-28509bc12522"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R-squared for 45C02: 0.6622459308494442\n",
            "R-squared for 25C05: 0.986778428637118\n",
            "R-squared for 45C01: 0.9423434826149811\n",
            "R-squared for 25C07: 0.21491218019955316\n",
            "R-squared for 25C04: 0.18225645357123077\n",
            "R-squared for 25C06: 0.5626068374885229\n",
            "R-squared for 25C01: 0.9403743470251013\n",
            "R-squared for 25C08: 0.423122585797846\n",
            "R-squared for 25C03: 0.6213505222028429\n",
            "R-squared for 25C02: 0.851300733780944\n",
            "R-squared for 35C01: 0.8953666083002053\n",
            "R-squared for 35C02: 0.8863437862028222\n"
          ]
        }
      ],
      "source": [
        "# List of battery files\n",
        "a = glob.glob(\"*.xlsx\")\n",
        "\n",
        "# Loop through each battery file\n",
        "for m in a:\n",
        "    # Load the battery data\n",
        "    df10 = pd.read_excel(m)\n",
        "    df101 = df10.drop(columns=['label', 'cycle number', 'Unnamed: 0', \"Capacity/mA.h\"])\n",
        "\n",
        "    # Initialize with the first 'sequence_length' capacity values for prediction\n",
        "    true_capacity = list(df10[\"Capacity/mA.h\"])  # True capacity values for R-squared calculation\n",
        "    predicted_capacity = true_capacity[:sequence_length]  # First 'sequence_length' true values to start the prediction\n",
        "\n",
        "    # Iterating through the battery data for predictions with adjustable sequence length\n",
        "    for i in range(len(df101) - (sequence_length - 1)):\n",
        "        # Create a sequence of 'sequence_length' steps (the model expects input of shape (sequence_length, 121))\n",
        "        seq = [list(df101.iloc[i + n]) for n in range(sequence_length)]\n",
        "\n",
        "        # Insert the true capacity values into the sequence at the appropriate places\n",
        "        for n in range(sequence_length):\n",
        "            seq[n].insert(0, predicted_capacity[i + n])  # Insert the corresponding capacity value\n",
        "\n",
        "        # Convert the sequence into a numpy array and reshape it to (1, sequence_length, 121)\n",
        "        seq = np.array(seq).reshape(1, sequence_length, 121)\n",
        "\n",
        "        # Make the prediction using the Liquid Neural Network model\n",
        "        predict = model1(tf.convert_to_tensor(seq, dtype=tf.float32))\n",
        "\n",
        "        # Append the predicted capacity value\n",
        "        predicted_capacity.append(predict.numpy()[0][0])\n",
        "\n",
        "    # Since predicted_capacity has extra initial values, slice it to match true_capacity length\n",
        "    predicted_capacity = predicted_capacity[:len(true_capacity)]\n",
        "\n",
        "    # Compute R-squared value for the predictions\n",
        "    r_squared = r2_score(true_capacity, predicted_capacity)\n",
        "    print(f\"R-squared for {m.split('.')[0]}: {r_squared}\")\n",
        "\n",
        "    # Save the predicted results for this battery\n",
        "    results = pd.DataFrame({'True Capacity': true_capacity, 'Predicted Capacity': predicted_capacity})\n",
        "    with pd.ExcelWriter(\"/content/content/\" + m.split('.')[0] + \"_results_with_prediction.xlsx\") as writer:\n",
        "        results.to_excel(writer, index=False)\n",
        "\n",
        "    # Plot the true and predicted capacity values for this battery\n",
        "    plt.figure()  # Create a new figure for each battery\n",
        "    plt.plot(true_capacity, label='True Capacity', color='blue')\n",
        "    plt.plot(predicted_capacity, label='Predicted Capacity', color='red')\n",
        "    plt.title(f\"True vs Predicted Capacity over Cycles for {m.split('.')[0]}\")\n",
        "    plt.xlabel(\"Cycle Number\")\n",
        "    plt.ylabel(\"Capacity (mA.h)\")\n",
        "    plt.legend()\n",
        "    plt.savefig(f\"{m.split('.')[0]}_Capacity_True_vs_Predicted.png\")  # Save the plot as a PNG file\n",
        "    plt.close()  # Close the plot to avoid overlap with the next one"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G28joP6nQZ1K"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}